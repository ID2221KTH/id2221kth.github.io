
                                                
                                                                                                
                                                                                                
<div class="center" 
>
<!--l. 83--><p class="noindent" >
<!--l. 83--><p class="noindent" ><span 
class="phvb8t-x-x-180">Containers and Virtual Machines at Scale: A Comparative</span>
<span 
class="phvb8t-x-x-180">Study</span>
<div class="tabular"> <table id="TBL-1" class="tabular" 
cellspacing="0" cellpadding="0"  
><colgroup id="TBL-1-1g"><col 
id="TBL-1-1"></colgroup><tr  
 style="vertical-align:baseline;" id="TBL-1-1-"><td  style="white-space:nowrap; text-align:center;" id="TBL-1-1-1"  
class="td11"></td></tr></table>
</div><!--tex4ht:inline--><div class="tabular"> <table id="TBL-2" class="tabular" 
cellspacing="0" cellpadding="0"  
><colgroup id="TBL-2-1g"><col 
id="TBL-2-1"></colgroup><tr  
 style="vertical-align:baseline;" id="TBL-2-1-"><td  style="white-space:wrap; text-align:left;" id="TBL-2-1-1"  
class="td11"><!--l. 83--><p class="noindent" ><span 
class="phvr8t-x-x-120">Prateek Sharma</span><sup class="textsuperscript"><span 
class="ptmr8t-x-x-80">1</span></sup><span 
class="phvr8t-x-x-120">, Lucas Chaufournier</span><sup class="textsuperscript"><span 
class="ptmr8t-x-x-80">1</span></sup><span 
class="phvr8t-x-x-120">, Prashant Shenoy</span><sup class="textsuperscript"><span 
class="ptmr8t-x-x-80">1</span></sup><span 
class="phvr8t-x-x-120">, Y.C. Tay</span><sup class="textsuperscript"><span 
class="ptmr8t-x-x-80">2</span></sup><br />
<span 
class="phvr8t-x-x-120">{prateeks, lucasch, shenoy}@cs.umass.edu, dcstayyc@nus.edu.sg</span><br />
<span 
class="phvr8t-">University of Massachusetts Amherst, USA</span><sup class="textsuperscript"><span 
class="ptmr8t-x-x-80">1</span></sup><span 
class="phvr8t-">, National University of Singapore, Singapore</span><sup class="textsuperscript"><span 
class="ptmr8t-x-x-80">2</span></sup>                             </td>
</tr></table>
</div>
</div>
                                                
                                                                                                
                                                                                                
<!--l. 83--><p class="indent" >  <hr class="float"><div class="float" 
>
                                                
                                                                                                
                                                                                                
<div class="center" 
>
<!--l. 83--><p class="noindent" >
<!--l. 83--><p class="center" ><img 
src="a1-sharma0x.png" alt="PICT" >
</div>
                                                
                                                                                                
                                                                                                
  </div><hr class="endfloat" />
<!--l. 85--><p class="noindent" ><span 
class="ptmb8t-x-x-120">ABSTRACT</span>
<!--l. 2--><p class="indent" >  Virtualization is used in data center and cloud environments to
decouple applications from the hardware they run on. Hardware
virtualization and operating system level virtualization are two
prominent technologies that enable this. Containers, which use OS
virtualization, have recently surged in interest and deployment. In this
paper, we study the differences between the two virtualization
technologies. We compare containers and virtual machines in large
data center environments along the dimensions of performance,
manageability and software development.
<!--l. 4--><p class="indent" >  We evaluate the performance differences caused by the different
virtualization technologies in data center environments where multiple
applications are running on the same servers (multi-tenancy). Our
results show that co-located applications can cause performance
interference, and the degree of interference is higher in the case of
containers for certain types of workloads. We also evaluate differences
in the management frameworks which control deployment and
orchestration of containers and VMs. We show how the different
capabilities exposed by the two virtualization technologies can
affect the management and development of applications. Lastly,
we evaluate novel approaches which combine hardware and OS
virtualization.
<!--l. 125--><p class="indent" >
<!--l. 125--><p class="noindent" ><span 
class="ptmb8t-x-x-120">CCS Concepts</span>
<!--l. 125--><p class="indent" >  <span 
class="ptmr8c-x-x-90">&#8226;</span><span 
class="ptmb8t-x-x-90">General and reference </span><span 
class="zptmcm7y-x-x-90">&#x2192;</span><span 
class="ptmb8t-x-x-90">Empirical studies; </span><span 
class="ptmri8t-x-x-90">Evaluation;</span>
<span 
class="ptmr8c-x-x-90">&#8226;</span><span 
class="ptmb8t-x-x-90">Computer systems organization </span><span 
class="zptmcm7y-x-x-90">&#x2192;</span><span 
class="ptmb8t-x-x-90">Cloud computing; </span><span 
class="ptmr8c-x-x-90">&#8226;</span><span 
class="ptmb8t-x-x-90">Software</span>
<span 
class="ptmb8t-x-x-90">and its engineering </span><span 
class="zptmcm7y-x-x-90">&#x2192;</span><span 
class="ptmri8t-x-x-90">Operating systems; </span>Cloud computing;
<!--l. 129--><p class="indent" >
<a 
 id="x1-2r1"></a>
<!--l. 129--><p class="noindent" ><span 
class="ptmb8t-x-x-120">1.</span>  <span 
class="ptmb8t-x-x-120">INTRODUCTION</span><a 
 id="Q1-1-0"></a>
.
<!--l. 1--><p class="indent" >  Modern enterprises increasingly rely on IT applications for their
business needs. Today&#8217;s enterprise IT applications are hosted in data
centers&#8212;servers and storage that provide compute, storage and network
resources to these applications. Modern data centers are increasingly
virtualized where by applications are hosted on one or more virtual
machines that are then mapped onto physical servers in the data
center.
<!--l. 3--><p class="indent" >  Virtualization provides a number of benefits. It enables a flexible
allocation of physical resources to virtualized applications where the
mapping of virtual to physical resources as well as the amount of
resources to each application can be varied dynamically to adjust to
changing application workloads. Furthermore, virtualization enables
multi-tenancy, which allows multiple instances of virtualized
applications (&#8220;tenants&#8221;) to share a physical server. Multi-tenancy allows
data centers to consolidate and pack applications into a smaller set of
servers and reduce operating costs. Virtualization also simplifies
replication and scaling of applications.
<!--l. 6--><p class="indent" >  There are two types of server virtualization technologies that are
common in data center environments&#8212;hardware-level virtualization and
operating system level virtualization. Hardware level virtualization
involves running a hypervisor which virtualizes the server&#8217;s resources
across multiple virtual machines. Each hardware virtual machine
(VM) runs its own operating system and applications. By contrast,
operating system virtualization virtualizes resources at the OS level.
OS-level virtualization encapsulates standard OS processes and their
dependencies to create &#8220;containers&#8221;, which are collectively managed by
the underlying OS kernel. Examples of hardware virtualization include
Xen&#x00A0;<span class="cite">[<span 
class="ptmb8t-x-x-90">?</span>]</span>, KVM&#x00A0;<span class="cite">[<span 
class="ptmb8t-x-x-90">?</span>]</span>, and VMware ESX&#x00A0;<span class="cite">[<span 
class="ptmb8t-x-x-90">?</span>]</span>. Operating system          virtualization is used by Linux containers (LXC&#x00A0;<span class="cite">[<span 
class="ptmb8t-x-x-90">?</span>]</span>), Ubuntu
     LXD&#x00A0;<span class="cite">[<span 
class="ptmb8t-x-x-90">?</span>]</span>, Docker&#x00A0;<span class="cite">[<span 
class="ptmb8t-x-x-90">?</span>]</span>, BSD Jails&#x00A0;<span class="cite">[<span 
class="ptmb8t-x-x-90">?</span>]</span>, Solaris Zones&#x00A0;<span class="cite">[<span 
class="ptmb8t-x-x-90">?</span>]</span> and Windows
     Containers&#x00A0;<span class="cite">[<span 
class="ptmb8t-x-x-90">?</span>]</span>.
     <!--l. 8--><p class="indent" >    Both types of virtualization technologies also have management
     frameworks that enable VMs and applications to be deployed and
     managed at data center scale. Examples of VM management
     frameworks include commercial offerings like vCenter&#x00A0;<span class="cite">[<span 
class="ptmb8t-x-x-90">?</span>]</span> and
     open source frameworks like OpenStack&#x00A0;<span class="cite">[<span 
class="ptmb8t-x-x-90">?</span>]</span>, CloudStack&#x00A0;<span class="cite">[<span 
class="ptmb8t-x-x-90">?</span>]</span>.
     Kubernetes&#x00A0;<span class="cite">[<span 
class="ptmb8t-x-x-90">?</span>]</span> and Docker Swarm&#x00A0;<span class="cite">[<span 
class="ptmb8t-x-x-90">?</span>]</span> are recent container management
     frameworks.
     <!--l. 11--><p class="indent" >    While hardware virtualization has been the predominant
     virtualization technology for deploying, packaging, and managing
     applications; containers (which use operating system virtualization) are
     increasingly filling that role due to the popularity of systems like
     Docker&#x00A0;<span class="cite">[<span 
class="ptmb8t-x-x-90">?</span>]</span>. Containers promise low-overhead virtualization and
     improved performance when compared to VMs. Despite the surge
     of interest in containers in enterprise environments, there is a
     distinct lack of performance comparison studies which quantify and
     compare the performance benefits of containers and VMs. Previous
     research&#x00A0;<span class="cite">[<span 
class="ptmb8t-x-x-90">?</span>,&#x00A0;<span 
class="ptmb8t-x-x-90">?</span>]</span> has compared the two technologies for single server
     environments, and our work builds on past work by examining
     performance in the presence of interference and also focuses on
     multi-server deployments that are common in cluster and data center
     environments.
     <!--l. 18--><p class="indent" >    Given these trends, in this paper we ask the following questions:
               <ol  class="enumerate1" >
               <li 
  class="enumerate" id="x1-4x1">When deploying applications in a data center environment,
               what                                        are                                        the
               advantages and disadvantages of each virtualization platform
               with regards to application performance, manageability and
               deployment at scale?
                                                                                                
                                                                                                
    </li>
    <li 
  class="enumerate" id="x1-6x2">Under what scenarios is one technology more suitable than
    the other?</li></ol>
<!--l. 28--><p class="indent" >  To answer these questions, we conduct a detailed comparison of
hardware and OS virtualization. While some of our results and
observations are specific to the idiosyncrasies of the platforms
we chose for our experimental evaluation, our goal is to derive
general results that are broadly applicable to the two types of
virtualization technologies. We choose open source platforms for our
evaluation&#8212;Linux containers (LXC) and KVM (a Linux-based
type-2 hypervisor) , and our method involves comparing four
configurations that are common in data center environments:
bare-metal, containers, virtual machines, and containers inside
VMs.
<!--l. 30--><p class="indent" >  Our comparative study asks these specific questions:
    <ol  class="enumerate1" >
    <li 
  class="enumerate" id="x1-8x1">How do the two virtualization approaches compare from a
    resource isolation and overcommitment perspective?
    </li>
    <li 
  class="enumerate" id="x1-10x2">How does each approach compare from the perspective of
    deploying many applications in VMs/containers at scale?
    </li>
    <li 
  class="enumerate" id="x1-12x3">How does each virtualization approach compare with respect
    to the application lifecycle and developer interaction?
    </li>
    <li 
  class="enumerate" id="x1-14x4">Can  approaches  which  combine  these  two  technologies
    (containers inside VMs and lightweight VMs) enable the best
    of both technologies to be reached?
    </li></ol>
<!--l. 49--><p class="indent" >  Our results show that co-located applications can cause performance
interference, and the degree of interference is higher in the case of
containers for certain types of workloads (Section&#x00A0;<a 
href="#x1-24r4">4<!--tex4ht:ref: sec:perf --></a>). We also evaluate
differences in the management frameworks which control deployment
and orchestration of containers and VMs (Section&#x00A0;<a 
href="#x1-45r5">5<!--tex4ht:ref: sec:managing --></a>). We show how the
different capabilities exposed by the two virtualization technologies can
affect the management and development of applications (Section&#x00A0;<a 
href="#x1-55r6">6<!--tex4ht:ref: sec:softengg --></a>).
Lastly, we evaluate novel approaches which combine hardware and OS
virtualization (Section&#x00A0;<a 
href="#x1-62r7">7<!--tex4ht:ref: sec:mix --></a>).
<a 
 id="x1-15r2"></a>
<!--l. 132--><p class="noindent" ><span 
class="ptmb8t-x-x-120">2.</span>  <span 
class="ptmb8t-x-x-120">BACKGROUND</span><a 
 id="Q1-1-0"></a>
.
<!--l. 1--><p class="indent" >  In this section we provide some background on the two types of
virtualization technologies that we study in this paper.
<a 
 id="x1-16r1"></a>
<!--l. 5--><p class="noindent" ><span 
class="ptmb8t-x-x-120">2.1</span>  <span 
class="ptmb8t-x-x-120">Hardware Virtualization</span><a 
 id="Q1-1-0"></a>
.
<!--l. 6--><p class="indent" >  Hardware virtualization involves virtualizing the hardware on a server
and creating virtual machines that provide the abstraction of a physical
machine. Hardware virtualization involves running a hypervisor,
also referred to as a virtual machine monitor (VMM), on the
bare metal server. The hypervisor emulates virtual hardware such
as the CPU, memory, I/O, and network devices for each virtual
machine. Each VM then runs an independent operating system and
applications on top of that OS. The hypervisor is also responsible for
multiplexing the underlying physical resources across the resident
VMs.                                                 <!--l. 8--><p class="indent" >    Modern hypervisors support multiple strategies for resource
     allocation and sharing of physical resources. Physical resources may be
     strictly partitioned (dedicated) to each VM, or shared in a best effort
     manner. The hypervisor is also responsible for isolation. Isolation
     among VMs is provided by trapping privileged hardware access by
     guest operating systems and performing those operations in the
     hypervisor on behalf of the guest OS. Examples of hardware
     virtualization platforms include VMware ESXi, Linux KVM and
     Xen.
     <a 
 id="x1-17r2"></a>
     <!--l. 10--><p class="noindent" ><span 
class="ptmb8t-x-x-120">2.2</span>    <span 
class="ptmb8t-x-x-120">Operating System Virtualization</span> <a 
 id="Q1-1-0"></a> 
.
     <!--l. 12--><p class="indent" >    <hr class="figure"><div class="figure" 
>
                                                                                                
                                                                                                
                                                
                                                                                                
                                                                                                
<a 
 id="x1-18r1"></a>
<!--l. 15--><p class="noindent" > <img 
src="../figs/a1-sharma1x.png" alt="PIC" class="graphics"><!--tex4ht:graphics  
name="a1-sharma1x.png" src="../figs/vmfig_fixed.pdf"  
-->
(a) Virtual Machines
<a 
 id="x1-19r2"></a> <img 
src="a1-sharma2x.png" alt="PIC" class="graphics"><!--tex4ht:graphics  
name="a1-sharma2x.png" src="../figs/containerfig_fixed.pdf"  
-->
(b) Containers                <a 
 id="x1-20r1"></a>
  <span 
class="ptmb8t-x-x-90">Figure 1: Hardware and operating system virtualization.</span>
                                                
                                                                                                
                                                                                                
<!--l. 21--><p class="indent" >  </div><hr class="endfigure">
<!--l. 24--><p class="indent" >  Operating system virtualization involves virtualizing the OS kernel
rather than the physical hardware (Figure&#x00A0;<a 
href="#x1-20r1">1<!--tex4ht:ref: fig:arch-all --></a>). OS-level virtual machines
are referred to as containers. Each container encapsulates a group of
processes that are isolated from other containers or processes
in the system. The OS kernel is responsible for implementing
the container abstraction. It allocates CPU shares, memory and
network I/O to each container and can also provide file system
isolation.
<!--l. 26--><p class="indent" >  Similar to hardware virtualization, different allocation strategies may
be supported such as dedicated, shared and best effort. Containers
provide lightweight virtualization since they do not run their own OS
kernels, but instead rely on the underlying kernel for OS services. In
some cases, the underlying OS kernel may emulate a different OS kernel
version to processes within a container. This is a feature often used to
support backward OS compatibility or emulating different OS APIs
such as in LX branded zones&#x00A0;<span class="cite">[<span 
class="ptmb8t-x-x-90">?</span>]</span> on Solaris and in running linux
applications on windows&#x00A0;<span class="cite">[<span 
class="ptmb8t-x-x-90">?</span>]</span>.
<!--l. 30--><p class="indent" >  Many OS virtualization techniques exist including Solaris Zones,
BSD-jails and Linux LXC. The recent emergence of Docker, a container
platform similar to LXC but with a layered filesystem and added
software engineering benefits, has renewed interest in container-based
virtualization for data centers and the cloud. Linux containers in
particular employ two key features:
<!--l. 33--><p class="noindent" ><span 
class="ptmb8t-x-x-90">Cgroups. </span>Control groups&#x00A0;<span class="cite">[<span 
class="ptmb8t-x-x-90">?</span>]</span> are a kernel mechanism for controlling
the resource allocation to process groups. Cgroups exist for each major
resource type: CPU, memory, network, block-IO, and devices. The
resource allocation for each of these can be controlled individually,
allowing the complete resource limits for a process or a process group to
be specified.
<!--l. 35--><p class="noindent" ><span 
class="ptmb8t-x-x-90">Namespaces. </span>A namespace provides an abstraction for a kernel
resource that makes it appear to the container that it has its own private,
isolated instance of the resource. In Linux, there are namespaces for
isolating: process IDs, user IDs, file system mount points, networking
interfaces, IPC, and host names&#x00A0;<span class="cite">[<span 
class="ptmb8t-x-x-90">?</span>]</span>.
<a 
 id="x1-21r3"></a>
<!--l. 37--><p class="noindent" ><span 
class="ptmb8t-x-x-120">2.3</span>  <span 
class="ptmb8t-x-x-120">Virtualized Data Centers</span><a 
 id="Q1-1-1"></a>
.
<!--l. 38--><p class="indent" >  While hardware and operating system level virtualization operates at
the granularity of a single server, data centers are comprised of large
clusters of servers, each of which is virtualized. Consequently, data
centers must rely on management frameworks that enable virtualized
resources of a cluster of servers to be managed efficiently.
<!--l. 40--><p class="indent" >  Such management frameworks simplify the placement and mapping
of VMs onto physical machines, enable VMs to be moved from one
machine to another (for load balancing) or allow for VMs to be resized
(to adjust to dynamic workloads). Frameworks also support service
orchestration, configuration management and automation of cluster
management tasks. Examples of popular management frameworks for
hardware virtualization include OpenStack&#x00A0;<span class="cite">[<span 
class="ptmb8t-x-x-90">?</span>]</span> &amp; VMware vCenter&#x00A0;<span class="cite">[<span 
class="ptmb8t-x-x-90">?</span>]</span>
while for OS-level virtualization there exist platforms such as
Kubernetes&#x00A0;<span class="cite">[<span 
class="ptmb8t-x-x-90">?</span>]</span> and Docker Swarm&#x00A0;<span class="cite">[<span 
class="ptmb8t-x-x-90">?</span>]</span>.
<a 
 id="x1-22r3"></a>
<!--l. 139--><p class="noindent" ><span 
class="ptmb8t-x-x-120">3.</span>  <span 
class="ptmb8t-x-x-120">PROBLEM STATEMENT</span><a 
 id="Q1-1-1"></a>
.
<!--l. 4--><p class="indent" >  The goal of our work is to conduct a comparative study of
hardware and OS-level virtualization from the perspective of a
data center. Some qualitative differences between the two are
apparent.
<!--l. 6--><p class="indent" >  OS-level virtualization is lightweight in nature and the emergence of
platforms like Docker have brought numerous advantages from an
application development and deployment standpoint. VMs are          considered to be more heavyweight but provide more robust isolation
     across untrusted co-resident VMs. Furthermore, while both hardware
     and OS-level virtualization have been around for decades, the same is
     not true for their management frameworks.
     <!--l. 8--><p class="indent" >    Management frameworks for hardware virtualization such as vCenter
     and OpenStack have been around for longer and have acquired more
     functionality over the years. In contrast, OS-level management
     frameworks such as Kubernetes are newer and less mature but are
     evolving rapidly.
     <!--l. 10--><p class="indent" >    From a data center perspective, it is interesting to study what kinds of
     scenarios are more suitable for hardware virtualization or OS-level
     virtualization. In particular, our evaluation is guided by the following
     research questions:
               <ul class="itemize1">
               <li class="itemize">What are the trade-offs between virtualization platforms on
               a  single  server  with  regards  to  application  performance,
               resource allocation and resource isolation?
               </li>
               <li class="itemize">What are the trade-offs of the two techniques when allocating
               resources from a cluster perspective?
               </li>
               <li class="itemize">What are the benefits from the perspective of deployment and
               application development process?
               </li>
               <li class="itemize">Can the two virtualization techniques be combined to provide
               high performance and ease of deployment/development?
               </li></ul>
     <!--l. 23--><p class="indent" >    A summary of our evaluation of the virtualization platforms can be
     found in Figure <a 
href="#x1-23r2">2<!--tex4ht:ref: fig:mapping --></a>.
     <!--l. 25--><p class="indent" >    <hr class="figure"><div class="figure" 
>
                                                                                                
                                                                                                
                                                
                                                                                                
                                                                                                
<!--l. 28--><p class="noindent" ><img 
src="a1-sharma3x.png" alt="PIC" class="graphics"><!--tex4ht:graphics  
name="a1-sharma3x.png" src="../figs/SOVU.pdf"  
-->
<a 
 id="x1-23r2"></a>
<span 
class="ptmb8t-x-x-90">Figure                2:                Evaluation                map                of</span>
<span 
class="ptmb8t-x-x-90">virtualization  platform  performance.  Shaded  areas  represent</span>
<span 
class="ptmb8t-x-x-90">where a platform&#8217;s capabilities outperforms the other.</span>
                                                
                                                                                                
                                                                                                
<!--l. 32--><p class="indent" >  </div><hr class="endfigure">
<a 
 id="x1-24r4"></a>
<!--l. 142--><p class="noindent" ><span 
class="ptmb8t-x-x-120">4.</span>  <span 
class="ptmb8t-x-x-120">SINGLE MACHINE PERFORMANCE</span><a 
 id="Q1-1-2"></a>
.
<!--l. 3--><p class="indent" >  In this section, we compare the single-machine performance of
containers and VMs. Our focus is to highlight the performance of
different workload types under various deployment scenarios. Prior
work on containers and VM performance&#x00A0;<span class="cite">[<span 
class="ptmb8t-x-x-90">?</span>,&#x00A0;<span 
class="ptmb8t-x-x-90">?</span>]</span> has focused on
comparing the performance of both of these platforms in isolation&#8212;the
host is only running one instance of the application. Instead, we
consider the performance of applications as they are deployed in data
center and cloud environments. The two primary characteristics of these
environments are multi-tenancy and overcommitment. Multi-tenancy
arises when multiple applications are deployed on shared hardware
resources. Data centers and cloud platforms may also <span 
class="ptmri8t-x-x-90">overcommit</span>
their hardware resources by running applications with resource
requirements that exceed available capacity. Both multi-tenancy and
overcommitment are used to increase consolidation and reduce the
operating costs in clouds and data centers. Therefore, for our
performance comparison of containers and VMs, we also focus on
multi-tenancy and overcommitment scenarios, in addition to the study
of the virtualization overheads when the applications are running in
isolation.
<!--l. 11--><p class="indent" >  In all our experiments, we use KVM&#x00A0;<span class="cite">[<span 
class="ptmb8t-x-x-90">?</span>]</span> (a type-2 hypervisor based
on Linux) for running VMs, and LXC&#x00A0;<span class="cite">[<span 
class="ptmb8t-x-x-90">?</span>]</span> for running containers. This
allows us to use the same Linux kernel and reduces the differences in
the software stacks when comparing the two platforms, and tease out the
differences between OS and hardware virtualization. Since virtual
machine performance can be affected by hardware and hypervisor
features, we restrict our evaluation to using hardware virtualization
features that are present in standard default KVM installations.
Wherever applicable, we will point to additional hypervisor and
hardware features that have shown to reduce virtualization overheads in
specific scenarios.
<!--l. 20--><p class="noindent" ><span 
class="ptmb8t-x-x-90">Methodology. </span>We configured both containers and VMs in such a
way that they are comparable environments and are allocated the
same amount of CPU and memory resources. We configured each
LXC container to use two cores, each pinned to a single core
on the host CPU. We set a hard limit of 4 GB of memory and
used bridged networking for public IP addresses. We configured
each KVM VM to use 2 cores, 4GB of memory and a 50GB
hard disk image. We configured the VMs to use virtIO for both
network and disk I/O and used a bridged networking interface
with TAP for network connectivity. The guest operating system
for the VMs is Ubuntu 14.04.3 with a 3.19 Linux kernel. The
LXC containers also use the same Ubuntu 14.04.3 userspace
libraries (since they are containers, the kernel is shared with the
host).
<!--l. 23--><p class="noindent" ><span 
class="ptmb8t-x-x-90">Setup. </span>The hardware platform for all our experiments is a Dell
PowerEdge R210 II server with a 4 core 3.40GHz E3-1240 v2 Intel
Xeon CPU, 16GB memory, and a 1 TB 7200 RPM disk. We disabled
hyperthreading to reduce the effects of hardware scheduling and
improve the stability of our results. The host ran on Ubuntu 14.04.3 (64
bit) with a 3.19 Linux Kernel. For virtualization we used LXC version
1.0.7 and QEMU with KVM version 2.0.0.
<!--l. 26--><p class="noindent" ><span 
class="ptmb8t-x-x-90">Workloads. </span>We use these workloads which stress different resources
(CPU, memory, disk, network):
<!--l. 28--><p class="noindent" ><span 
class="ptmb8t-x-x-90">Filebench. </span>We use the customizable file system benchmark filebench
v.1.4.91 with its randomrw workload to test file IO performance. The
randomrw workload allocates a 5Gb file and then spawns two threads to
work on the file, one for reads and one for writes. We use the default
8KB IO size.                                            <!--l. 30--><p class="noindent" ><span 
class="ptmb8t-x-x-90">Kernel-compile. </span>We use the Linux kernel compile benchmark to test
     the CPU performance by measuring the runtime of compiling
     Linux-4.2.2 with the default configuration and multiple threads (equal to
     the number of available cores).
     <!--l. 33--><p class="noindent" ><span 
class="ptmb8t-x-x-90">SpecJBB. </span>SpecJBB2005 is a popular CPU and memory intensive
     benchmark that emulates a three tier web application stack and exercises
     the underlying system supporting it.
     <!--l. 35--><p class="noindent" ><span 
class="ptmb8t-x-x-90">RUBiS. </span>RUBiS is a multi-tier web application that emulates the popular
     auction site eBay. We run RUBiS version 1.4.3 with three guests: one
     with the Apache and PHP frontend, one with the RUBiS backend
     MySQL database and one with the RUBiS client and workload
     generator.
     <!--l. 39--><p class="noindent" ><span 
class="ptmb8t-x-x-90">YCSB. </span>YCSB is a workload generator developed by Yahoo to test
     different key value stores used in the cloud. YCSB provides statistics on
     the performance of load, insert, update and read operations. We use
     YCSB version 0.4.0 with Redis version 3.0.5 key value store.
     We use a YCSB workload which contains 50% reads and 50%
     writes.
     <a 
 id="x1-25r1"></a>
     <!--l. 43--><p class="noindent" ><span 
class="ptmb8t-x-x-120">4.1</span>    <span 
class="ptmb8t-x-x-120">Baseline Performance</span> <a 
 id="Q1-1-2"></a> 
.
     <!--l. 45--><p class="indent" >    We first measure the virtualization overhead when only a single
     application is running on a physical host. This allows us to observe and
     measure the performance overhead imposed by the virtualization layer.
     We run the same workload, and configure the containers and the
     VMs to use the same amount of CPU and memory resources. We
     shall show the performance of CPU, memory, and I/O intensive
     workloads.
     <!--l. 49--><p class="indent" >    Because of virtualizing at the OS layer, running inside a container
     does not add any noticeable overhead compared to running the same
     application on the bare-metal OS. As alluded to in Section&#x00A0;<a 
href="#x1-15r2">2<!--tex4ht:ref: background --></a>,
     running an application inside a container involves two differences
     when compared to running it as a conventional OS process (or a
     group of processes). The first is that containers need resource
     accounting to enforce resource limits. This resource accounting is
     also done for the various operations that the kernel performs on
     behalf of the application (handling system calls, caching directory
     entries, etc), and adds only a minimal overhead. The other aspect of
     containers is isolation, which is provided in Linux by namespaces for
     processes, users, etc. Namespaces provide a virtualized view of some
     kernel resources like processes, users, mount-points etc, and the
     number of extra kernel-mode operations involved is again limited.
     We can thus think of containers in this case as extensions of the
     <span 
class="aett9-">u-limit </span>and <span 
class="aett9-">r-limit</span>&#x00A0;<span class="cite">[<span 
class="ptmb8t-x-x-90">?</span>]</span> functionality. Our experiments (Figure&#x00A0;<a 
href="#x1-26r3">3<!--tex4ht:ref: fig:bm --></a>)
     did not yield any noticeable difference between bare-metal and
     LXC performance, and for ease of exposition, we assume that
     the bare-metal performance of applications is the same as with
     LXC.
     <!--l. 52--><p class="indent" >    <hr class="figure"><div class="figure" 
>
                                                                                                
                                                                                                
                                                
                                                                                                
                                                                                                
<!--l. 54--><p class="noindent" ><img 
src="a1-sharma4x.png" alt="PIC" class="graphics"><!--tex4ht:graphics  
name="a1-sharma4x.png" src="../graphs/baseline/baremetal.pdf"  
--> <a 
 id="x1-26r3"></a>
<span 
class="ptmb8t-x-x-90">Figure 3: LXC performance relative to bare metal is within 2%.</span>
                                                
                                                                                                
                                                                                                
<!--l. 57--><p class="indent" >  </div><hr class="endfigure">
                                                
                                                                                                
                                                                                                
<!--l. 61--><p class="indent" >  <hr class="float"><div class="float" 
>
                                                
                                                                                                
                                                                                                
<a 
 id="x1-27r1"></a>
 <img 
src="a1-sharma5x.png" alt="PIC" class="graphics"><!--tex4ht:graphics  
name="a1-sharma5x.png" src="../graphs/baseline/cpu.pdf"  
-->
(a) CPU intensive                        <a 
 id="x1-28r2"></a> <img 
src="a1-sharma6x.png" alt="PIC" class="graphics"><!--tex4ht:graphics  
name="a1-sharma6x.png" src="../graphs/baseline/mem.pdf"  
-->
                                    (b) Memory intensive              <a 
 id="x1-29r3"></a> <img 
src="a1-sharma7x.png" alt="PIC" class="graphics"><!--tex4ht:graphics  
name="a1-sharma7x.png" src="../graphs/baseline/disk.pdf"  
-->
                                                                 (c) Disk intensive
<a 
 id="x1-30r4"></a> <img 
src="a1-sharma8x.png" alt="PIC" class="graphics"><!--tex4ht:graphics  
name="a1-sharma8x.png" src="../graphs/baseline/net.pdf"  
-->
(d) Network intensive       <a 
 id="x1-31r4"></a>
<span 
class="ptmb8t-x-x-90">Figure 4: Performance overhead of KVM is negligible for our CPU and memory intensive workloads, but high in case of I/O intensive</span>
<span 
class="ptmb8t-x-x-90">applications. Unlike CPU and memory operations, I/O operations go through the hypervisor&#8212;contributing to their high overhead.</span>
                                                
                                                                                                
                                                                                                
  </div><hr class="endfloat" />
<!--l. 78--><p class="noindent" ><span 
class="ptmb8t-x-x-90">CPU. </span>Figure&#x00A0;<a 
href="#x1-27r1">4a<!--tex4ht:ref: fig:cpu-base --></a> shows the difference in performance for CPU intensive
workloads. The performance difference when running on VMs vs.
LXCs is under 3% (LXC fares slightly better). Thus, the hardware
virtualization overhead for CPU intensive workloads is small, which is
in part due to virtualization support in the CPU (VMX instructions and
two dimensional paging) to reduce the number of traps to the hypervisor
in case of privileged instructions.
<!--l. 81--><p class="noindent" ><span 
class="ptmb8t-x-x-90">Memory. </span>Figure&#x00A0;<a 
href="#x1-28r2">4b<!--tex4ht:ref: fig:mem-base --></a> shows the performance of Redis in-memory
key-value store under the YCSB benchmark. For the load, read, and
update operations, the VM latency is around 10% higher as compared to
LXC.
<!--l. 86--><p class="noindent" ><span 
class="ptmb8t-x-x-90">Disk. </span>For testing disk I/O performance, we use the filebench randomrw
workload. Because all guest I/O must go through the hypervisor when
using virtIO, the VM I/O performance is expected to be worse than
LXC. Figure&#x00A0;<a 
href="#x1-29r3">4c<!--tex4ht:ref: fig:disk-base --></a> shows the throughput and latency for the filebench
benchmark. The disk throughput and latency for VMs are 80% worse
for the randomrw test. The randomrw filebench test issues lots of small
reads and writes, and each one of them has to be handled by a single
hypervisor thread. I/O workloads running inside VMs that are more
amenable to caching and buffering show better performance, and we
chose the randomrw workload as the worst-case workload for
virtIO.
<!--l. 90--><p class="noindent" ><span 
class="ptmb8t-x-x-90">Network. </span>We use the RUBiS benchmark described earlier to measure
network performance of guests. For RUBiS, we do not see a noticeable
difference in the performance between the two virtualization techniques
(Figure&#x00A0;<a 
href="#x1-30r4">4d<!--tex4ht:ref: fig:net-base --></a>).
<!--l. 97--><p class="noindent" ><span 
class="ptmb8t-x-x-90">Summary of baseline results: </span><span 
class="ptmri8t-x-x-90">The performance overhead of hardware</span>
<span 
class="ptmri8t-x-x-90">virtualization is low when the application does not have to go through</span>
<span 
class="ptmri8t-x-x-90">the hypervisor, as is the case of CPU and memory operations.</span>
<span 
class="ptmri8t-x-x-90">Throughput and latency of I/O intensive applications can suffer even</span>
<span 
class="ptmri8t-x-x-90">with paravirtualized I/O.</span>
<a 
 id="x1-32r2"></a>
<!--l. 101--><p class="noindent" ><span 
class="ptmb8t-x-x-120">4.2</span>  <span 
class="ptmb8t-x-x-120">Performance Isolation</span><a 
 id="Q1-1-4"></a>
.
<!--l. 105--><p class="indent" >  So far, we have shown the performance overheads of virtualization
when only a single application is running on the physical host. However,
multiple applications of different types and belonging to different
users are often co-located on the same physical host to increase
consolidation. In this subsection, we measure the performance
interference due to co-located applications running inside VMs and
containers.
<!--l. 107--><p class="indent" >  When measuring this &#8220;noisy neighbor&#8221; effect, we are interested in
seeing how one application affects the performance of another. We shall
compare the performance of applications when co-located with a variety
of neighbors versus their stand-alone performance. In all our
experiments, the VMs and containers are configured with the same
amount of CPU and memory resources. Since the application
performance depends on the co-located applications, we compare
the application performance for a diverse range of co-located
applications:
<span 
class="ptmb8t-x-x-90">Competing.</span> These co-located applications are contending for the
    same  resource.  For  example,  if  our  target  application  is
    CPU-intensive,  then  the  other  co-located  applications  are
    also CPU-intensive. This tests how well the platform layer
    (OS or the hypervisor) is able to partition and multiplex the
    resources among multiple claimants.
<span 
class="ptmb8t-x-x-90">Orthogonal.</span> In  this  scenario,  the  co-located  applications  seek
    different   resources.   For   example,   if   one   application   is                 CPU-intensive,  the  other  one  is  network  intensive.  This
               scenario   is   likely   when   the   scheduling   and   placement
               middleware prefers to co-locate non-competing applications
               on  the  same  host  to  minimize  resource  contention  and
               improve consolidation.
     <span 
class="ptmb8t-x-x-90">Adversarial.</span>  In this scenario, the other co-located application is
               a misbehaving, adversarial application which tries to cause
               the other application to be starved of resources. In contrast to
               the earlier configuration, these adversarial applications may
               not represent realistic workloads, but may arise in practice if
               multiple users are allowed to run their applications on shared
               hardware, and these applications present a vector for a denial
               of resource attack.
     <a 
 id="x1-33r1"></a>
     <!--l. 118--><p class="noindent" > <span 
class="ptmri8t-x-x-110">4.2.1</span>    <span 
class="ptmri8t-x-x-110">CPU Isolation</span> <a 
 id="Q1-1-4"></a>
    .
     <!--l. 120--><p class="indent" >    <hr class="figure"><div class="figure" 
>
                                                                                                
                                                                                                
                                                
                                                                                                
                                                                                                
<!--l. 122--><p class="noindent" ><img 
src="a1-sharma9x.png" alt="PIC" class="graphics"><!--tex4ht:graphics  
name="a1-sharma9x.png" src="../graphs/interf/cpu-set.pdf"  
--> <a 
 id="x1-34r5"></a>
<span 
class="ptmb8t-x-x-90">Figure  5:  CPU  interference  is  higher  for  LXC  even  with</span>
<span 
class="ptmb8t-x-x-90">CPU-sets,  especially  since  adversarial  workloads  can  cause</span>
<span 
class="ptmb8t-x-x-90">co-located applications to be starved of resources and not finish</span>
<span 
class="ptmb8t-x-x-90">execution (DNF: did not finish).</span>
                                                
                                                                                                
                                                                                                
<!--l. 126--><p class="indent" >  </div><hr class="endfigure">
<!--l. 128--><p class="indent" >  To measure the performance interference for CPU bound workloads,
we compare the running time of the kernel compile application when it
is co-located with other applications relative to its stand-alone
performance without any interference. We use another instance of the
kernel compile workload to induce CPU contention, SpecJBB as the
orthogonal workload , and a fork-bomb as the adversarial workload
scenario. The fork bomb is a simple script that overloads the process
table by continually forking processes in an infinite loop. Figure&#x00A0;<a 
href="#x1-34r5">5<!--tex4ht:ref: fig:cpu-set --></a>
shows the kernel compile performance relative to the no interference
case for LXC and VMs. In the case of LXC, there are two ways of
allocating CPU resources. The LXC containers can either be assigned to
CPU-cores (<span 
class="aett9-">cpu-sets</span>), or the containers can be multiplexed across all
CPU cores in a fair-share manner by the Linux CPU scheduler
(<span 
class="aett9-">cpu-shares</span>).
<!--l. 130--><p class="indent" >  The same amount of CPU resources were allocated in both the
CPU-shares and CPU-sets cases&#8212;50% CPU and 2 out of 4 cores
respectively. Despite this, running containers with CPU-shares results
in a greater amount of interference, of up to 60% higher when
compared to the baseline case of stand-alone no-interference
performance.
<!--l. 133--><p class="indent" >  From Figure&#x00A0;<a 
href="#x1-34r5">5<!--tex4ht:ref: fig:cpu-set --></a>, we see that when co-located with the adversarial
fork-bomb workload, the LXC containers are starved of resources and
do not finish in any reasonable amount of time, while the VM manages
to finish with a 30% performance degradation. While the fork-bomb test
may be an extreme adversarial workload, we note that the leaky
container abstraction which does not provide careful accounting and
control of every physical <span 
class="ptmri8t-x-x-90">and </span>kernel operation can cause many more
such cases to arise.
<!--l. 144--><p class="noindent" ><span 
class="ptmb8t-x-x-90">Result: </span><span 
class="ptmri8t-x-x-90">Interference for CPU-bound workloads is mitigated by</span>
<span 
class="ptmri8t-x-x-90">hypervisors because of separate CPU schedulers in the guest operating</span>
<span 
class="ptmri8t-x-x-90">systems. The shared host OS kernel can cause performance disruption</span>
<span 
class="ptmri8t-x-x-90">in the case of containers, and can potentially lead to denial of</span>
<span 
class="ptmri8t-x-x-90">service.</span>
<a 
 id="x1-35r2"></a>
<!--l. 147--><p class="noindent" > <span 
class="ptmri8t-x-x-110">4.2.2</span>  <span 
class="ptmri8t-x-x-110">Memory Isolation</span><a 
 id="Q1-1-5"></a>
.
<!--l. 149--><p class="indent" >  <hr class="figure"><div class="figure" 
>
                                                
                                                                                                
                                                                                                
                                                
                                                                                                
                                                                                                
<!--l. 151--><p class="noindent" ><img 
src="a1-sharma10x.png" alt="PIC" class="graphics"><!--tex4ht:graphics  
name="a1-sharma10x.png" src="../graphs/interf/mem.pdf"  
--> <a 
 id="x1-36r6"></a>
<span 
class="ptmb8t-x-x-90">Figure  6:  Performance  interference  is  limited  for  memory</span>
<span 
class="ptmb8t-x-x-90">intensive  workloads,  although  LXC  suffers  more  with  the</span>
<span 
class="ptmb8t-x-x-90">adversarial (malloc-bomb) workload.</span>
                                                
                                                                                                
                                                                                                
<!--l. 155--><p class="indent" >  </div><hr class="endfigure">
<!--l. 157--><p class="indent" >  To measure performance interference for memory based workloads,
we compare SpecJBB throughput against its baseline performance. For
the competing case we ran an additional instance of SpecJBB, and
used kernel compile as an orthogonal workload. To illustrate
an adversarial workload we used a malloc bomb, in an infinite
loop, that incrementally allocates memory until it runs out of
space.
<!--l. 161--><p class="indent" >  Figure&#x00A0;<a 
href="#x1-36r6">6<!--tex4ht:ref: fig:mem1 --></a> shows the memory interference result and shows that
memory isolation provided by containers is sufficient for most
uses. Both the competing and orthogonal workloads for VMs
and LXC are well within a reasonable range of their baseline
performance. In the adversarial case however, it appears that the
VM outperforms LXC. LXC sees a performance decrease of
32% where as the VM only suffers a performance decrease of
11%.
<a 
 id="x1-37r3"></a>
<!--l. 166--><p class="noindent" > <span 
class="ptmri8t-x-x-110">4.2.3</span>  <span 
class="ptmri8t-x-x-110">Disk Isolation</span><a 
 id="Q1-1-6"></a>
.
<!--l. 171--><p class="indent" >  For disk interference, we compare filebench&#8217;s baseline performance
against its performance while running alongside the three types of
interference workloads. We chose the following workloads as
neighbors: a second instance of filebench for the competing case, kernel
compile for orthogonal and an instance of Bonnie++, a benchmark that
runs lots of small reads and writes, for the adversarial case. The
containers were configured with equal block-IO cgroup weights to
ensure equal I/O bandwidth allocation. Figure&#x00A0;<a 
href="#x1-38r7">7<!--tex4ht:ref: fig:diskI --></a> shows the disk
interference result.
<!--l. 173--><p class="indent" >  <hr class="figure"><div class="figure" 
>
                                                
                                                                                                
                                                                                                
                                                
                                                                                                
                                                                                                
<!--l. 175--><p class="noindent" ><img 
src="a1-sharma11x.png" alt="PIC" class="graphics"><!--tex4ht:graphics  
name="a1-sharma11x.png" src="../graphs/interf/disk.pdf"  
--> <a 
 id="x1-38r7"></a>
<span 
class="ptmb8t-x-x-90">Figure  7:  Disk  performance  interference  is  high  for  both</span>
<span 
class="ptmb8t-x-x-90">containers and virtual machines.</span>
                                                
                                                                                                
                                                                                                
<!--l. 179--><p class="indent" >  </div><hr class="endfigure">
<!--l. 182--><p class="indent" >  For LXC, the latency increases 8 times. For VMs, the latency
increase is only 2x. This reduction can be attributed to the fact
that the disk I/O performance for VMs is quite bad even in the
isolated case (Figure&#x00A0;<a 
href="#x1-29r3">4c<!--tex4ht:ref: fig:disk-base --></a>), and raw disk bandwidth is still available
for other VMs to use. However, a reduction of 8x in the case of
containers is still significant, and points to the lack of disk I/O
isolation.
<!--l. 185--><p class="noindent" ><span 
class="ptmb8t-x-x-90">Result: </span><span 
class="ptmri8t-x-x-90">Sharing the host OS block layer components like the I/O</span>
<span 
class="ptmri8t-x-x-90">scheduler increases the performance interference for disk bound</span>
<span 
class="ptmri8t-x-x-90">workloads in containers. VMs may offer more isolation and shield better</span>
<span 
class="ptmri8t-x-x-90">against noisy neighbors, but the effectiveness of hypervisors in</span>
<span 
class="ptmri8t-x-x-90">mitigating I/O interference is reduced because of shared I/O paths for</span>
<span 
class="ptmri8t-x-x-90">guest I/O.</span>
<a 
 id="x1-39r4"></a>
<!--l. 195--><p class="noindent" > <span 
class="ptmri8t-x-x-110">4.2.4</span>  <span 
class="ptmri8t-x-x-110">Network Isolation</span><a 
 id="Q1-1-7"></a>
.
<!--l. 198--><p class="indent" >  To measure performance interference for network intensive
workloads, we compare RUBiS throughput against its baseline
performance while running co-located with the three types of interfering
workloads. To measure competing interference we run RUBiS alongside
the YCSB benchmark while we use SpecJBB to measure orthogonal
interference. To measure adversarial interference performance we run a
guest that is the receiver of a UDP bomb. The guest runs a UDP server
while being flooded with small UDP packets in an attempt to overload
the shared network interface. Figure&#x00A0;<a 
href="#x1-40r8">8<!--tex4ht:ref: fig:networkI --></a> shows the network interference
results. For each type of workload, there is no significant difference in
interference.
<!--l. 201--><p class="indent" >  <hr class="figure"><div class="figure" 
>
                                                
                                                                                                
                                                                                                
                                                
                                                                                                
                                                                                                
<!--l. 203--><p class="noindent" ><img 
src="a1-sharma12x.png" alt="PIC" class="graphics"><!--tex4ht:graphics  
name="a1-sharma12x.png" src="../graphs/interf/network.pdf"  
--> <a 
 id="x1-40r8"></a>
<span 
class="ptmb8t-x-x-90">Figure  8:  Network  performance  interference  when  running</span>
<span 
class="ptmb8t-x-x-90">RUBiS is similar for both containers and virtual machines.</span>
                                                
                                                                                                
                                                                                                
<!--l. 207--><p class="indent" >  </div><hr class="endfigure">
<!--l. 218--><p class="noindent" ><span 
class="ptmb8t-x-x-90">Summary of interference results: </span><span 
class="ptmri8t-x-x-90">Shared hypervisors or operating</span>
<span 
class="ptmri8t-x-x-90">system components can reduce performance isolation, especially with</span>
<span 
class="ptmri8t-x-x-90">competing and adversarial workloads in the case of containers.</span>
<a 
 id="x1-41r3"></a>
<!--l. 221--><p class="noindent" ><span 
class="ptmb8t-x-x-120">4.3</span>  <span 
class="ptmb8t-x-x-120">Overcommitment</span><a 
 id="Q1-1-8"></a>
.
<!--l. 223--><p class="indent" >  To improve packing efficiency, it is often necessary to overcommit the
physical resources. Containers use overcommitment mechanisms built
into the operating system itself, such as CPU overcommitment by
multiplexing and memory overcommit by relying on the virtual
memory subsystem&#8217;s swapping and paging. Hypervisors provide
overcommitment by multiplexing the virtual hardware devices onto
the actual physical hardware. The overcommitment provided by
hypervisors may be of reduced effectiveness because the guest
operating system may be in the dark about the resources that have
been taken from the VM. For example, the hypervisor might
preempt a vCPU of a VM at the wrong time when it is holding
locks. Such lock holder and lock waiter preemptions can degrade
performance for multi-threaded applications&#x00A0;<span class="cite">[<span 
class="ptmb8t-x-x-90">?</span>]</span>. We investigate how
application performance behaves in resource overcommitment
scenarios.
<!--l. 225--><p class="indent" >  <hr class="figure"><div class="figure" 
>
                                                
                                                                                                
                                                                                                
                                                
                                                                                                
                                                                                                
<a 
 id="x1-42r1"></a>
<!--l. 228--><p class="noindent" ><img 
src="a1-sharma13x.png" alt="PIC" class="graphics"><!--tex4ht:graphics  
name="a1-sharma13x.png" src="../graphs/overcommit/cpu_8k.pdf"  
-->
(a) CPU intensive
<a 
 id="x1-43r2"></a><img 
src="a1-sharma14x.png" alt="PIC" class="graphics"><!--tex4ht:graphics  
name="a1-sharma14x.png" src="../graphs/overcommit/spec_15.pdf"  
-->
(b) Memory intensive
<a 
 id="x1-44r9"></a>
<span 
class="ptmb8t-x-x-90">Figure 9: Kernel compile and SpecJBB performance of VMs</span>
<span 
class="ptmb8t-x-x-90">when the CPU is overcommitted by a factor of 1.5.</span>
                                                
                                                                                                
                                                                                                
<!--l. 235--><p class="indent" >  </div><hr class="endfigure">
<!--l. 239--><p class="indent" >  Figure&#x00A0;<a 
href="#x1-44r9">9<!--tex4ht:ref: fig:all-overcommit --></a> compares the relative performance of LXC and VMs in an
overcommitment scenario where both the CPU and memory have been
oversubscribed by a factor of 1.5. CPU overcommitment is handled in
both hypervisors and operating systems by multiplexing multiple
vCPUs and processes onto CPU cores, and for the CPU intensive kernel
compile workload, VM performance is within 1% of LXC performance
(Figure&#x00A0;<a 
href="#x1-42r1">9a<!--tex4ht:ref: fig:cpu-overcommit --></a>).
<!--l. 242--><p class="indent" >  Memory overcommitment in hypervisors is more challenging,
because virtual machines are allocated a fixed memory size upon their
creation, and overcommitting memory involves &#8220;stealing&#8221; pages
from the VM via approaches like host-swapping or ballooning.
For the more memory intensive SpecJBB workload (Figure&#x00A0;<a 
href="#x1-43r2">9b<!--tex4ht:ref: fig:mem-overcommit --></a>),
the VM performs about 10% worse compared to LXC. Memory
overcommitment for VMs can also be improved by approaches like
page deduplication&#x00A0;<span class="cite">[<span 
class="ptmb8t-x-x-90">?</span>,&#x00A0;<span 
class="ptmb8t-x-x-90">?</span>,&#x00A0;<span 
class="ptmb8t-x-x-90">?</span>]</span> which reduce the effective memory
footprint of VMs by sharing pages, or by using approaches like
transcendent memory&#x00A0;<span class="cite">[<span 
class="ptmb8t-x-x-90">?</span>]</span>, which can provide more efficient in-memory
compressed storage of evicted pages.
<!--l. 248--><p class="noindent" ><span 
class="ptmb8t-x-x-90">Result: </span><span 
class="ptmri8t-x-x-90">Hypervisors handle CPU overcommitment more gracefully than</span>
<span 
class="ptmri8t-x-x-90">memory overcommitment, because vCPUs can be multiplexed onto CPU</span>
<span 
class="ptmri8t-x-x-90">cores dynamically.</span>
<a 
 id="x1-45r5"></a>
<!--l. 146--><p class="noindent" ><span 
class="ptmb8t-x-x-120">5.</span>  <span 
class="ptmb8t-x-x-120">CLUSTER MANAGEMENT</span><a 
 id="Q1-1-9"></a>
.
<!--l. 3--><p class="indent" >  In this section, we illustrate the differences in managing containers
and VMs at scale. Cluster operators seek to satisfy the resource
requirements of all applications and increase consolidation to reduce
the operating costs. In the rest of this section, we shall see how
the different characteristics of containers and VMs affect the
different options for managing and provisioning resources in a
cluster. For comparison purposes, we use VMware vCenter&#x00A0;<span class="cite">[<span 
class="ptmb8t-x-x-90">?</span>]</span> and
OpenStack&#x00A0;<span class="cite">[<span 
class="ptmb8t-x-x-90">?</span>]</span> as representative VM management platforms,
and Kubernetes&#x00A0;<span class="cite">[<span 
class="ptmb8t-x-x-90">?</span>]</span> as a representative container orchestration
system.
<a 
 id="x1-46r1"></a>
<!--l. 7--><p class="noindent" ><span 
class="ptmb8t-x-x-120">5.1</span>  <span 
class="ptmb8t-x-x-120">Resource Allocation</span><a 
 id="Q1-1-9"></a>
.
<!--l. 9--><p class="indent" >  Data center and cloud operators use cluster management frameworks
to fulfill the resource requirements of applications. We focus on the
physical resource requirements of applications, and note that handling
application-SLA based requirements is generally outside the scope of
management frameworks. Cluster management software rely on
the resource provisioning &#8220;knobs&#8221; that the underlying platforms
(hypervisors and operating systems) provide for managing the resource
allocation of VMs and containers. The hypervisor controls the allocation
of the virtual CPU, memory and I/O devices for each VM. Since VMs
can be thought of as sharing the &#8220;raw hardware&#8221;, the resource
allocation is also of that granularity. For example, VMs may be
allocated a fixed number of virtual CPUs, memory, and I/O devices
and bandwidth. For CPU and I/O bandwidth, fair-share or other
policies for resource multiplexing may be implemented by the
hypervisor.
                                                
                                                                                                
                                                                                                
<!--l. 11--><p class="indent" >  <hr class="float"><div class="float" 
>
                                                
                                                                                                
                                                                                                
<div class="tabular"> <table id="TBL-3" class="tabular" 
cellspacing="0" cellpadding="0" rules="groups" 
><colgroup id="TBL-3-1g"><col 
id="TBL-3-1"></colgroup><colgroup id="TBL-3-2g"><col 
id="TBL-3-2"></colgroup><colgroup id="TBL-3-3g"><col 
id="TBL-3-3"></colgroup><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-3-1-"><td  style="white-space:nowrap; text-align:center;" id="TBL-3-1-1"  
class="td11">             </td><td  style="white-space:wrap; text-align:left;" id="TBL-3-1-2"  
class="td11"><!--l. 15--><p class="noindent" >KVM                             </td><td  style="white-space:wrap; text-align:left;" id="TBL-3-1-3"  
class="td11"><!--l. 15--><p class="noindent" >LXC/Docker                                                                                                                                   </td></tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-3-2-"><td  style="white-space:nowrap; text-align:center;" id="TBL-3-2-1"  
class="td11"> CPU </td><td  style="white-space:wrap; text-align:left;" id="TBL-3-2-2"  
class="td11"><!--l. 17--><p class="noindent" >VCPU count </td><td  style="white-space:wrap; text-align:left;" id="TBL-3-2-3"  
class="td11"><!--l. 17--><p class="noindent" >CPU-set/CPU-shares, cpu-period, cpu-quota,</td>
</tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-3-3-"><td  style="white-space:nowrap; text-align:center;" id="TBL-3-3-1"  
class="td11">   Memory       </td><td  style="white-space:wrap; text-align:left;" id="TBL-3-3-2"  
class="td11"><!--l. 19--><p class="noindent" >Virtual RAM size          </td><td  style="white-space:wrap; text-align:left;" id="TBL-3-3-3"  
class="td11"><!--l. 19--><p class="noindent" >Memory soft/hard limit, kernel memory, overcommitment options, shared-memory size, swap
size, swappiness                                                                                                                             </td>
</tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-3-4-"><td  style="white-space:nowrap; text-align:center;" id="TBL-3-4-1"  
class="td11">     I/O           </td><td  style="white-space:wrap; text-align:left;" id="TBL-3-4-2"  
class="td11"><!--l. 21--><p class="noindent" >virtIO, SR-IOV             </td><td  style="white-space:wrap; text-align:left;" id="TBL-3-4-3"  
class="td11"><!--l. 21--><p class="noindent" >Blkio read/write weights, priorities                                                                                                </td>
</tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-3-5-"><td  style="white-space:nowrap; text-align:center;" id="TBL-3-5-1"  
class="td11"> Security Policy  </td><td  style="white-space:wrap; text-align:left;" id="TBL-3-5-2"  
class="td11"><!--l. 23--><p class="noindent" >None                             </td><td  style="white-space:wrap; text-align:left;" id="TBL-3-5-3"  
class="td11"><!--l. 23--><p class="noindent" >Privilege levels, Capabilities(kernel modules, nice, resource limits, setuid)                                 </td>
</tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-3-6-"><td  style="white-space:nowrap; text-align:center;" id="TBL-3-6-1"  
class="td11">   Volumes       </td><td  style="white-space:wrap; text-align:left;" id="TBL-3-6-2"  
class="td11"><!--l. 25--><p class="noindent" >Virtual disks                  </td><td  style="white-space:wrap; text-align:left;" id="TBL-3-6-3"  
class="td11"><!--l. 25--><p class="noindent" >File-system paths                                                                                                                            </td>
</tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-3-7-"><td  style="white-space:nowrap; text-align:center;" id="TBL-3-7-1"  
class="td11">Environment vars</td><td  style="white-space:wrap; text-align:left;" id="TBL-3-7-2"  
class="td11"><!--l. 27--><p class="noindent" >N/A                               </td><td  style="white-space:wrap; text-align:left;" id="TBL-3-7-3"  
class="td11"><!--l. 27--><p class="noindent" >Entry scripts                                                                                                                                   </td>
</tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-3-8-"><td  style="white-space:nowrap; text-align:center;" id="TBL-3-8-1"  
class="td11">             </td></tr></table></div><a 
 id="x1-47r1"></a>
           <span 
class="ptmb8t-x-x-90">Table 1: Configuration options available for LXC and KVM. Containers have more options available.</span>
                                                
                                                                                                
                                                                                                
  </div><hr class="endfloat" />
<!--l. 35--><p class="indent" >  With containers, resource allocation involves more dimensions, since
the resource control knobs provided by the operating system are
larger in number. In addition to physical resource allocation like
CPU and memory, other kernel resources for controlling CPU
scheduling, swapping, etc can also be specified for containers (Table&#x00A0;<a 
href="#x1-47r1">1<!--tex4ht:ref: tab:config --></a>).
Thus, provisioning for containers involves allocation of physical
<span 
class="ptmri8t-x-x-90">and </span>operating system resources. Not limiting a certain resource
may lead to its excessive consumption, and cause performance
interference, as we have seen in Section&#x00A0;<a 
href="#x1-32r2">4.2<!--tex4ht:ref: subsec:interference --></a>. As we have also
seen, containers have multiple resource allocation options for
the same resource (cpu-sets and cpu-shares), and their selection
can have a non-negligible impact on performance, as shown in
Figure&#x00A0;<a 
href="#x1-48r10">10<!--tex4ht:ref: fig:cpu-shares --></a>. We see that the SpecJBB throughput differs by up
to 40% when the container is allocated <img 
src="a1-sharma15x.png" alt="1
4"  class="frac" align="middle">th of cpu cores using
cpu-sets, when compared to the equivalent allocation of 25% with
cpu-shares.
<!--l. 38--><p class="indent" >  <hr class="figure"><div class="figure" 
>
                                                
                                                                                                
                                                                                                
                                                
                                                                                                
                                                                                                
<!--l. 40--><p class="noindent" ><img 
src="a1-sharma16x.png" alt="PIC" class="graphics"><!--tex4ht:graphics  
name="a1-sharma16x.png" src="../graphs/baseline/cgroup.pdf"  
--> <a 
 id="x1-48r10"></a>
<span 
class="ptmb8t-x-x-90">Figure  10:  CPU-shares  vs.  CPU-sets  allocation  can  have  a</span>
<span 
class="ptmb8t-x-x-90">significant impact on application performance, even though the</span>
<span 
class="ptmb8t-x-x-90">same amount of CPU resources are allocated in each instance.</span>
                                                
                                                                                                
                                                                                                
<!--l. 44--><p class="indent" >  </div><hr class="endfigure">
<!--l. 50--><p class="indent" >  In addition to the increased number of dimensions required to specify
resource requirements, container resource management faces
cross-platform challenges. With VMs, abstraction layers like libVirt&#x00A0;<span class="cite">[<span 
class="ptmb8t-x-x-90">?</span>]</span>
allow management frameworks like OpenStack to run VMs on multiple
hypervisors. Policies for controlling and provisioning VMs can be
applied across hypervisors. However, in the case of containers, the
resource control interfaces are heavily operating system dependent, and
running containers across operating systems may prove to be a
challenge. Efforts like the Open Container Initiative and CNCF (Cloud
Native Computing Foundation) have been launched to develop a
standards for container formats, resource and lifecycle control. This will
allow frameworks like Kubernetes to specify resource allocation and
control containers of different types (Docker, rkt, etc) across different
operating systems.
<!--l. 63--><p class="indent" >  <span 
class="ptmb8t-x-x-90">Soft and hard limits. </span>A fundamental difference in resource
allocation with containers is the prevalence of soft limits on resources.
Soft limits enable applications to use resources <span 
class="ptmri8t-x-x-90">beyond </span>their allocated
limit if those resources are under-utilized. In the case of virtual
machines, resource limits are generally <span 
class="ptmri8t-x-x-90">hard</span>&#8212;the VM cannot utilize
more resources than its allocation even if these resources are idle on
the host. Dynamically increasing resource allocation to VMs is
fundamentally a hard problem. VMs are allocated virtual hardware
(CPUs, memory, I/O devices) before boot-up. Adding virtual devices
during guest execution requires the guest operating system to
support some form of <span 
class="ptmri8t-x-x-90">device hotplug</span>. Because of the rare nature of
hotplugging CPU and memory in the physical environment, hotplug
support for operating systems is limited, and therefore not a feasible
option in most cases. Virtualization specific interfaces such as
transcendent memory&#x00A0;<span class="cite">[<span 
class="ptmb8t-x-x-90">?</span>]</span> can be used to dynamically increase
effective VM memory, but current VM management frameworks do
not support it as it requires co-operation between the VMs and
the hypervisor, which is often hard to guarantee in mulit-tenant
environments.
<!--l. 71--><p class="indent" >  <hr class="figure"><div class="figure" 
>
                                                
                                                                                                
                                                                                                
                                                
                                                                                                
                                                                                                
<a 
 id="x1-49r1"></a>
<!--l. 74--><p class="noindent" > <img 
src="a1-sharma17x.png" alt="PIC" class="graphics"><!--tex4ht:graphics  
name="a1-sharma17x.png" src="../graphs/lxcvm/softlimit.pdf"  
-->
(a)   Performance   with   hard   vs.   soft
limits   when   CPU   and   memory   are
overcommitted by a factor of 1.5.
<a 
 id="x1-50r2"></a> <img 
src="a1-sharma18x.png" alt="PIC" class="graphics"><!--tex4ht:graphics  
name="a1-sharma18x.png" src="../graphs/overcommit/spec_8.pdf"  
-->
(b)                              Performance
of VMs vs. soft-limited containers
at overcommitment factor of 2.  <a 
 id="x1-51r11"></a>
<span 
class="ptmb8t-x-x-90">Figure 11: Soft limits in containers improve overcommitment</span>
<span 
class="ptmb8t-x-x-90">by allowing under utilized resources to be used by applications</span>
<span 
class="ptmb8t-x-x-90">that need them the most.</span>
                                                
                                                                                                
                                                                                                
<!--l. 80--><p class="indent" >  </div><hr class="endfigure">
<!--l. 84--><p class="indent" >  By allowing applications to use under-utilized resources, soft limits
may enable more efficient resource utilization. Soft limits are
particularly effective in overcommitted scenarios. Figure&#x00A0;<a 
href="#x1-49r1">11a<!--tex4ht:ref: fig:softlimit --></a> shows
scenarios where the CPU and memory for the containers have been
overcommitted by a factor of 1.5. In this scenario, the YCSB
latency is about 25% lower for read and update operations if the
containers are soft-limited. Similarly, Figure&#x00A0;<a 
href="#x1-50r2">11b<!--tex4ht:ref: fig:spec_8 --></a> shows results
from a scenario where the resources were overcommitted by a
factor of two. The containers were again soft-limited, and we
compare against VMs which have hard limits. We again see a
big improvement with soft limits, as the SpecJBB throughput
is 40% higher with the soft-limited containers compared to the
VMs.
<!--l. 87--><p class="noindent" ><span 
class="ptmb8t-x-x-90">Result: </span><span 
class="ptmri8t-x-x-90">Soft-limits enable containers to run with better performance on</span>
<span 
class="ptmri8t-x-x-90">overcommitted hosts. Soft-limits are inherently difficult with VMs</span>
<span 
class="ptmri8t-x-x-90">because their resource allocation is fixed during guest operating system</span>
<span 
class="ptmri8t-x-x-90">boot-up.</span>
<a 
 id="x1-52r2"></a>
<!--l. 105--><p class="noindent" ><span 
class="ptmb8t-x-x-120">5.2</span>  <span 
class="ptmb8t-x-x-120">Migration</span><a 
 id="Q1-1-12"></a>
.
<!--l. 107--><p class="indent" >  Live migration is an important mechanism to transfer the application
state from one host to another, and is used in data centers for load
balancing, consolidation and fault-tolerance. Live-migration for VMs
works by periodically copying memory pages from the source to the
destination host, and management platforms trigger migrations based on
migration policies which take into account the availability of resources
on the source and destination hosts. The duration of live migration
depends on the application characteristics (the page dirty rate) as well as
the memory footprint of the application.
<!--l. 109--><p class="indent" >  Virtual machine live migration is mature and widely used in data
centers, and frameworks like vCenter have sophisticated policies for
automatically moving VMs to balance load. Unlike VM migration,
container migration requires process migration techniques and is
not as reliable a mechanism. Container migration is harder to
implement in practice because of the large amount of operating
system state associated with a process (process control block,
file table, sockets, etc) which must be captured and saved along
with the memory pages. As a result, projects such as CRIU&#x00A0;<span class="cite">[<span 
class="ptmb8t-x-x-90">?</span>]</span>
(Checkpoint Restart In Userspace) have been attempting to provide
live-migration for containers, but the functionality is limited to a
small set of applications which use the supported subset of OS
services. Container migration is hence not mature (yet), and is not
supported by management frameworks. Instead of migration,
killing and restarting stateless containers is a viable option for
consolidation of containers. Furthermore, container migration depends
on the availability of many additional libraries and kernel features,
which may not be available on all the hosts. These dependencies
may limit the number of potential destination hosts for container
migration.
                                                
                                                                                                
                                                                                                
<!--l. 112--><p class="indent" >  <hr class="float"><div class="float" 
>
                                                
                                                                                                
                                                                                                
<div class="tabular"> <table id="TBL-4" class="tabular" 
cellspacing="0" cellpadding="0" rules="groups" 
><colgroup id="TBL-4-1g"><col 
id="TBL-4-1"></colgroup><colgroup id="TBL-4-2g"><col 
id="TBL-4-2"></colgroup><colgroup id="TBL-4-3g"><col 
id="TBL-4-3"></colgroup><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-4-1-"><td  style="white-space:nowrap; text-align:center;" id="TBL-4-1-1"  
class="td11"> Application   </td><td  style="white-space:wrap; text-align:left;" id="TBL-4-1-2"  
class="td11"><!--l. 115--><p class="noindent" >Container memory size       </td><td  style="white-space:nowrap; text-align:left;" id="TBL-4-1-3"  
class="td11">VM size</td>
</tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-4-2-"><td  style="white-space:nowrap; text-align:center;" id="TBL-4-2-1"  
class="td11">Kernel Compile</td><td  style="white-space:wrap; text-align:left;" id="TBL-4-2-2"  
class="td11"><!--l. 117--><p class="noindent" >0.42                                     </td><td  style="white-space:nowrap; text-align:left;" id="TBL-4-2-3"  
class="td11">4            </td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-4-3-"><td  style="white-space:nowrap; text-align:center;" id="TBL-4-3-1"  
class="td11">    YCSB        </td><td  style="white-space:wrap; text-align:left;" id="TBL-4-3-2"  
class="td11"><!--l. 118--><p class="noindent" >4                                          </td><td  style="white-space:nowrap; text-align:left;" id="TBL-4-3-3"  
class="td11">4            </td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-4-4-"><td  style="white-space:nowrap; text-align:center;" id="TBL-4-4-1"  
class="td11">   SpecJBB     </td><td  style="white-space:wrap; text-align:left;" id="TBL-4-4-2"  
class="td11"><!--l. 119--><p class="noindent" >1.7                                       </td><td  style="white-space:nowrap; text-align:left;" id="TBL-4-4-3"  
class="td11">4            </td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-4-5-"><td  style="white-space:nowrap; text-align:center;" id="TBL-4-5-1"  
class="td11">  Filebench     </td><td  style="white-space:wrap; text-align:left;" id="TBL-4-5-2"  
class="td11"><!--l. 120--><p class="noindent" >2.2                                       </td><td  style="white-space:nowrap; text-align:left;" id="TBL-4-5-3"  
class="td11">4            </td>
</tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-4-6-"><td  style="white-space:nowrap; text-align:center;" id="TBL-4-6-1"  
class="td11">            </td></tr></table></div><a 
 id="x1-53r2"></a>
<span 
class="ptmb8t-x-x-90">Table   2:   Memory   sizes   (in   Gigabytes)   which   have   to   be</span>
<span 
class="ptmb8t-x-x-90">migrated for various applications. The mapped memory which</span>
<span 
class="ptmb8t-x-x-90">needs to be migrated is significantly smaller for containers.</span>
                                                
                                                                                                
                                                                                                
  </div><hr class="endfloat" />
<!--l. 130--><p class="indent" >  Migrating VMs involves the transfer of both the application state and
the guest operating system state (including slab and file-system page
caches), which can lead to increased migration times compared to
migrating the application state alone. We compare the memory sizes of
various applications when deployed in a containers and KVM VMs in
Table&#x00A0;<a 
href="#x1-53r2">2<!--tex4ht:ref: tab:migration1 --></a>. In both the cases, the containers and VMs are configured with
the same memory hard-limit. Except for the YCSB workload, which
uses the in-memory Redis key-value store, the application memory
footprint with containers is about 50-90% smaller as compared to the
equivalent VMs.
<!--l. 134--><p class="noindent" ><span 
class="ptmb8t-x-x-90">Result: </span><span 
class="ptmri8t-x-x-90">The memory footprint of containers is smaller than VMs,</span>
<span 
class="ptmri8t-x-x-90">leading to potentially smaller migration times. But container migration</span>
<span 
class="ptmri8t-x-x-90">mechanisms are not currently mature enough, and this lack of maturity</span>
<span 
class="ptmri8t-x-x-90">along with the larger number of dependencies may limit their</span>
<span 
class="ptmri8t-x-x-90">functionality.</span>
<a 
 id="x1-54r3"></a>
<!--l. 153--><p class="noindent" ><span 
class="ptmb8t-x-x-120">5.3</span>  <span 
class="ptmb8t-x-x-120">Deployment</span><a 
 id="Q1-1-13"></a>
.
<!--l. 155--><p class="indent" >  Launching applications is the key feature provided by management
frameworks, and the ability to launch these applications at low
latency is an important requirement. This is especially the case for
containers whose use-cases often call for rapid deployment. Launching
applications requires the management platform to support and
implement many policies for provisioning hardware resources and
deploying the VMs/containers. A significant amount of effort has gone
into provisioning policies and mechanisms for VMs, and these policies
may or may not translate to containers.
<!--l. 159--><p class="indent" >  Management and orchestration frameworks must also provide policies
for application placement, which involves assigning applications to
physical hosts with sufficient resources available. Virtual machine
placement for consolidation has received significant attention&#x00A0;<span class="cite">[<span 
class="ptmb8t-x-x-90">?</span>]</span>. For
containers, which require a larger number of dimensions to specify
resource limits, existing placement and consolidation policies
may need to be modified and refined. Management platforms
also enforce co-location (affinity) constraints which ensure that
applications can be &#8220;bundled&#8221; and placed on the same physical host. In
Kubernetes, this is accomplished by using pods, which are groups of
containers and also function as the unit of deployment. As we
have shown earlier, containers suffer from larger performance
interference. Because of this concern, container placement might
need to be optimized to choose the right set of neighbors for each
application.
<!--l. 162--><p class="indent" >  By running multiple instances of an application, virtualization
enables easy horizontal scaling. The number of replicas of a container or
a VM can be specified to the management frameworks. Additionally,
Kubernetes also monitors for failed replicas and restarts failed replicas
automatically. Quickly launching application replicas to meet workload
demand is useful to handle load spikes etc. In the unoptimized
case, booting up virtual machines can take tens of seconds. By
contrast, container start times are well under a second&#x00A0;<span class="cite">[<span 
class="ptmb8t-x-x-90">?</span>]</span>. An
optimization to reduce the start-up latency of VMs is to employ fast
VM cloning&#x00A0;<span class="cite">[<span 
class="ptmb8t-x-x-90">?</span>]</span>. A similar functionality is provided by vCenter
linked-clones.
<!--l. 164--><p class="noindent" ><span 
class="ptmb8t-x-x-90">Multi-tenancy </span>An important aspect of virtualization is the ability to
share a cluster among multiple users. Due to hardware virtualization&#8217;s
strong resource isolation, multi-tenancy is common in virtual
machine environments. Because the isolation provided by containers
is weaker, multi-tenancy is considered too risky especially for
Linux containers. In cases where the isolation provided by the
OS is strong enough (as in the case of Solaris), containers have
been used for multi-tenancy in production environments&#x00A0;<span class="cite">[<span 
class="ptmb8t-x-x-90">?</span>]</span>. The        multi-tenant support for container management platforms like
     Kubernetes is under development but because of the security
     risks of sharing machines between untrusted users, policies for
     security-aware container placement may need to be developed.
     Unlike VMs which are &#8220;secure by default&#8221;, containers require
     several security configuration options (Table&#x00A0;<a 
href="#x1-47r1">1<!--tex4ht:ref: tab:config --></a>) to be specified for
     safe execution. In addition to satisfying resource constraints,
     management frameworks also need to verify and enforce these security
     constraints.
     <!--l. 183--><p class="noindent" ><span 
class="ptmb8t-x-x-90">Summary. </span>We have shown how the unique characteristics of VMs and
     containers lead to the different management capabilities of their
     respective management frameworks. As container technology
     matures, some of these are bound to change, and hence we have
     focused on the fundamental differences. There are opportunities and
     challenges for resource allocation in containers as a result of their
     richer resource allocation requirements; lack of live-migration, and
     multi-tenancy constraints due to security and performance isolation
     concerns.
     <a 
 id="x1-55r6"></a>
     <!--l. 149--><p class="noindent" ><span 
class="ptmb8t-x-x-120">6.</span>    <span 
class="ptmb8t-x-x-120">END-TO-END DEPLOYMENT</span> <a 
 id="Q1-1-13"></a> 
.
     <!--l. 3--><p class="indent" >    Performance is not the sole appeal of containers. Systems like Docker
     have surged in popularity by exploiting both the low deployment
     overheads and the improvements in the software development life cycle
     that containers enable. In this section, we look at how containers and
     VMs differ in the end-to-end deployment of applications&#8212;from
     the developer&#8217;s desktop/laptop to the production cloud or data
     center.
     <a 
 id="x1-56r1"></a>
     <!--l. 8--><p class="noindent" ><span 
class="ptmb8t-x-x-120">6.1</span>    <span 
class="ptmb8t-x-x-120">Constructing Images</span> <a 
 id="Q1-1-13"></a> 
.
     <!--l. 10--><p class="indent" >    Disk images encapsulate the code, libraries, data, and the environment
     required to run an application. Containers and VMs have different
     approaches to the image construction problem. In the case of
     VMs, constructing an image is really constructing a virtual disk
     which has the operating system, application libraries, application
     code and configuration, etc. The traditional way to build a virtual
     machine is to build from scratch which involves allocating blocks
     for a disk image and installing and configuring the operating
     system. Libraries and other dependencies of the application are
     then installed via the operating system&#8217;s package management
     toolchain.
     <!--l. 13--><p class="indent" >    Constructing VM images can be automated with systems like
     Vagrant&#x00A0;<span class="cite">[<span 
class="ptmb8t-x-x-90">?</span>]</span>, Chef, Puppet, etc., wherein the VM images are created by
     specifying configuration scripts and recipes to automate operating
     system configuration, application installation and service management.
     Additionally, cloud operators also provide virtual appliances&#8212;pre-built
     and pre-configured images for running specific applications (such as
     MySQL).
     <!--l. 19--><p class="indent" >    Containers implement a different approach due to their lightweight
     nature. Because containers can use the host file system, they do not need
     virtual disks (which is a block-level abstraction layer). Thus, a
     container &#8220;image&#8221; is simply a collection of files that an application
     depends on, and includes libraries and other dependencies of the
     application. In contrast to VMs, no operating system kernel is present in
     the container image, since containers use the host OS kernel.
     Similar to virtual appliances, pre-built container images for popular
     applications exist. Users can download these images and build off of
     them.
     <!--l. 22--><p class="indent" >    Constructing container images can also be automated in systems
     like Docker&#x00A0;<span class="cite">[<span 
class="ptmb8t-x-x-90">?</span>]</span> via dockerfiles. Container images can be built
     from existing ones in a deterministic and repeatable manner by
                                                                                                
                                                                                                
specifying the commands and scripts to build them in the dockerfile. In
contrast to VMs where creating images is handled by third party
tools like Vagrant, dockerfiles are an integral part of Docker, and
enable tighter integration between containers and their provenance
information.
<!--l. 25--><p class="indent" >  Due to differences in the contents and the sizes of constructed
images, the time to construct them are different for containers and VMs.
Table&#x00A0;<a 
href="#x1-57r3">3<!--tex4ht:ref: tab:vagrant --></a> shows the time to build images of popular applications when
the build process is specified via Vagrant and Docker. Building both
container and VM images involves downloading the base images
(containing the bare operating system) and then installing the required
software packages. The total time for creating the VM images is
about 2<span 
class="zptmcm7y-x-x-90">&#x00D7; </span>that of creating the equivalent container image. This
increase can be attributed to the extra time spent in downloading
and configuring the operating system that is required for virtual
machines.
                                                
                                                                                                
                                                                                                
<!--l. 52--><p class="indent" >  <hr class="float"><div class="float" 
>
                                                
                                                                                                
                                                                                                
<div class="tabular"> <table id="TBL-5" class="tabular" 
cellspacing="0" cellpadding="0" rules="groups" 
><colgroup id="TBL-5-1g"><col 
id="TBL-5-1"></colgroup><colgroup id="TBL-5-2g"><col 
id="TBL-5-2"></colgroup><colgroup id="TBL-5-3g"><col 
id="TBL-5-3"></colgroup><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-5-1-"><td  style="white-space:nowrap; text-align:center;" id="TBL-5-1-1"  
class="td11">Application</td><td  style="white-space:nowrap; text-align:left;" id="TBL-5-1-2"  
class="td11">Vagrant</td><td  style="white-space:nowrap; text-align:left;" id="TBL-5-1-3"  
class="td11">Docker</td>
</tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-5-2-"><td  style="white-space:nowrap; text-align:center;" id="TBL-5-2-1"  
class="td11">  MySQL   </td><td  style="white-space:nowrap; text-align:left;" id="TBL-5-2-2"  
class="td11">236.2   </td><td  style="white-space:nowrap; text-align:left;" id="TBL-5-2-3"  
class="td11">129      </td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-5-3-"><td  style="white-space:nowrap; text-align:center;" id="TBL-5-3-1"  
class="td11">  Nodejs    </td><td  style="white-space:nowrap; text-align:left;" id="TBL-5-3-2"  
class="td11">303.8   </td><td  style="white-space:nowrap; text-align:left;" id="TBL-5-3-3"  
class="td11">49        </td>
</tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-5-4-"><td  style="white-space:nowrap; text-align:center;" id="TBL-5-4-1"  
class="td11">         </td></tr></table></div><a 
 id="x1-57r3"></a>
<span 
class="ptmb8t-x-x-90">Table 3: Time (in seconds) to build an image using Vagrant (for</span>
<span 
class="ptmb8t-x-x-90">VMs) and Docker.</span>
                                                
                                                                                                
                                                                                                
  </div><hr class="endfloat" />
                                                
                                                                                                
                                                                                                
<!--l. 70--><p class="indent" >  <hr class="float"><div class="float" 
>
                                                
                                                                                                
                                                                                                
<div class="tabular"> <table id="TBL-6" class="tabular" 
cellspacing="0" cellpadding="0" rules="groups" 
><colgroup id="TBL-6-1g"><col 
id="TBL-6-1"></colgroup><colgroup id="TBL-6-2g"><col 
id="TBL-6-2"></colgroup><colgroup id="TBL-6-3g"><col 
id="TBL-6-3"></colgroup><colgroup id="TBL-6-4g"><col 
id="TBL-6-4"></colgroup><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-6-1-"><td  style="white-space:nowrap; text-align:left;" id="TBL-6-1-1"  
class="td11">Application</td><td  style="white-space:nowrap; text-align:left;" id="TBL-6-1-2"  
class="td11">VM      </td><td  style="white-space:nowrap; text-align:left;" id="TBL-6-1-3"  
class="td11">Docker </td><td  style="white-space:nowrap; text-align:left;" id="TBL-6-1-4"  
class="td11">Docker Incremental</td>
</tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-6-2-"><td  style="white-space:nowrap; text-align:left;" id="TBL-6-2-1"  
class="td11">MySQL       </td><td  style="white-space:nowrap; text-align:left;" id="TBL-6-2-2"  
class="td11">1.68GB</td><td  style="white-space:nowrap; text-align:left;" id="TBL-6-2-3"  
class="td11">0.37GB</td><td  style="white-space:nowrap; text-align:left;" id="TBL-6-2-4"  
class="td11">112KB                    </td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-6-3-"><td  style="white-space:nowrap; text-align:left;" id="TBL-6-3-1"  
class="td11">Nodejs         </td><td  style="white-space:nowrap; text-align:left;" id="TBL-6-3-2"  
class="td11">2.05GB</td><td  style="white-space:nowrap; text-align:left;" id="TBL-6-3-3"  
class="td11">0.66GB</td><td  style="white-space:nowrap; text-align:left;" id="TBL-6-3-4"  
class="td11">72KB                      </td>
</tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-6-4-"><td  style="white-space:nowrap; text-align:left;" id="TBL-6-4-1"  
class="td11">         </td></tr></table></div><a 
 id="x1-58r4"></a>
<span 
class="ptmb8t-x-x-90">Table   4:   Resulting   image   sizes   when   deploying   various</span>
<span 
class="ptmb8t-x-x-90">applications.  For  Docker,  we  also  measure  the  incremental</span>
<span 
class="ptmb8t-x-x-90">size when launching multiple containers belonging to the same</span>
<span 
class="ptmb8t-x-x-90">image.</span>
                                                
                                                                                                
                                                                                                
  </div><hr class="endfloat" />
<!--l. 86--><p class="indent" >  The differences in creating images discussed above influence another
important concern: image size. The resulting image sizes are also
smaller for containers, because they do not require an operating system
installation and additional overhead caused by the guest OS file system,
etc. This difference in image sizes is shown in Table&#x00A0;<a 
href="#x1-58r4">4<!--tex4ht:ref: tab:img-size --></a>, which compares
VM and container images for different applications. The smaller
container image sizes (by up to 3x) allows for faster deployment and
lower storage overhead.
<!--l. 96--><p class="noindent" ><span 
class="ptmb8t-x-x-90">Results: </span><span 
class="ptmri8t-x-x-90">Container images are faster to create and are smaller,</span>
<span 
class="ptmri8t-x-x-90">because the operating system and redundant libraries need not be</span>
<span 
class="ptmri8t-x-x-90">included.</span>
<a 
 id="x1-59r2"></a>
<!--l. 103--><p class="noindent" ><span 
class="ptmb8t-x-x-120">6.2</span>  <span 
class="ptmb8t-x-x-120">Version Control</span><a 
 id="Q1-1-15"></a>
.
<!--l. 106--><p class="indent" >  A novel feature that containers enable is the ability to version
control images. Just like version control for software, being able
to commit and track lineage of images is useful, and is a key
feature in Docker&#8217;s design. In the case of Docker, this versioning is
performed by using copy-on-write functionality (like AuFS) in the
host file system. Storing images in a copy-on-write file system
allows an image to be composed of multiple <span 
class="ptmri8t-x-x-90">layers</span>, with each
layer being immutable. The base layer comprises of the base
operating system, and modifications to existing images create
additional layers. In this way, multiple container images can share the
same physical files. Updates to files results in creation of new
layers.
<!--l. 108--><p class="indent" >  Virtual machine disks can also use layered storage in the form of
copy-on-write virtual disk formats (such as qcow2, FVD&#x00A0;<span class="cite">[<span 
class="ptmb8t-x-x-90">?</span>]</span>, etc.).
Unlike layered container images, virtual disks employ block level
copy-on-write instead of file-level. This finer granularity of versioning
creates a semantic decoupling between the user and the virtual disks&#8212;it
is harder to correlate changes in VM configurations with changes in the
the virtual disks. In the case of Docker, layers also store their ancestor
information and what commands were used to build the layer.
This allows Docker to have a semantically rich image versioning
tree.
                                                
                                                                                                
                                                                                                
<!--l. 110--><p class="indent" >  <hr class="float"><div class="float" 
>
                                                
                                                                                                
                                                                                                
<div class="tabular"> <table id="TBL-7" class="tabular" 
cellspacing="0" cellpadding="0" rules="groups" 
><colgroup id="TBL-7-1g"><col 
id="TBL-7-1"></colgroup><colgroup id="TBL-7-2g"><col 
id="TBL-7-2"></colgroup><colgroup id="TBL-7-3g"><col 
id="TBL-7-3"></colgroup><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-7-1-"><td  style="white-space:nowrap; text-align:center;" id="TBL-7-1-1"  
class="td11"> Workload  </td><td  style="white-space:nowrap; text-align:center;" id="TBL-7-1-2"  
class="td11">Docker</td><td  style="white-space:nowrap; text-align:center;" id="TBL-7-1-3"  
class="td11">VM</td></tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-7-2-"><td  style="white-space:nowrap; text-align:center;" id="TBL-7-2-1"  
class="td11">Dist Upgrade</td><td  style="white-space:nowrap; text-align:center;" id="TBL-7-2-2"  
class="td11"> 470 </td><td  style="white-space:nowrap; text-align:center;" id="TBL-7-2-3"  
class="td11">391</td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-7-3-"><td  style="white-space:nowrap; text-align:center;" id="TBL-7-3-1"  
class="td11">Kernel install</td><td  style="white-space:nowrap; text-align:center;" id="TBL-7-3-2"  
class="td11"> 292   </td><td  style="white-space:nowrap; text-align:center;" id="TBL-7-3-3"  
class="td11">303</td>
</tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-7-4-"><td  style="white-space:nowrap; text-align:center;" id="TBL-7-4-1"  
class="td11">          </td></tr></table></div><a 
 id="x1-60r5"></a>
<span 
class="ptmb8t-x-x-90">Table 5: Running time (in seconds) of operations in Docker and</span>
<span 
class="ptmb8t-x-x-90">VMs. Docker image layers cause increase in runtime for write</span>
<span 
class="ptmb8t-x-x-90">intensive applications.</span>
                                                
                                                                                                
                                                                                                
  </div><hr class="endfloat" />
<!--l. 124--><p class="indent" >  While layering enables versioning of images and allows images to be
easily composed, there are performance implications because of the
copy-on-write implications. Writes to a file in a layer causes a new copy
and a new layer to be created. We examine the performance overhead
induced by the copy on write layers in Table&#x00A0;<a 
href="#x1-60r5">5<!--tex4ht:ref: tab:layers1 --></a>, which shows
the running time of write-heavy workloads. Docker&#8217;s layered
storage architecture contributes results in an almost 40% slowdown
compared to VMs. This slow-down is almost entirely attributable to
the AuFS&#x00A0;<span class="cite">[<span 
class="ptmb8t-x-x-90">?</span>]</span> copy-on-write performance, and using other file
systems with more optimized copy-on-write functionality, like
ZFS, BtrFS, and OverlayFS can help bring the file-write overhead
down.
<!--l. 126--><p class="indent" >  Immutable copy-on-write layers also make cloning images extremely
fast, and this can be useful to deploy multiple containers from the same
image for rapid cloning and scaling. Starting multiple containers from a
single image is a lightweight operation, and takes less than one second
for most images. This is attributable to the reduced state size which is
associated with a container which needs to be copied instead of copying
an entire image. The incremental image size for a new Docker container
is shown in Table&#x00A0;<a 
href="#x1-58r4">4<!--tex4ht:ref: tab:img-size --></a>. To launch a new container, only &#x00A0;100KB of
extra storage space is required, compared to more than 3 GB for
VMs.
<!--l. 133--><p class="noindent" ><span 
class="ptmb8t-x-x-90">Result: </span><span 
class="ptmri8t-x-x-90">Although copy-on-write layers used for storing container</span>
<span 
class="ptmri8t-x-x-90">images aid in reducing build times and enable versioning, they may</span>
<span 
class="ptmri8t-x-x-90">reduce application performance, especially for disk I/O intensive</span>
<span 
class="ptmri8t-x-x-90">applications, because of the overhead of copy-on-write.</span>
<a 
 id="x1-61r3"></a>
<!--l. 138--><p class="noindent" ><span 
class="ptmb8t-x-x-120">6.3</span>  <span 
class="ptmb8t-x-x-120">Continuous Delivery/Integration</span><a 
 id="Q1-1-16"></a>
.
<!--l. 142--><p class="indent" >  Faster image construction, image versioning, and rapid cloning
have changed software engineering practices. Encapsulating all
software dependencies within an image provides a standardized
and convenient way of sharing and deploying software. Both
VM and container images are used for packaging application
software, but as we have discussed, the smaller image footprint
and the semantically rich image versioning has made container
images (especially Docker images) an increasingly popular way to
package and deploy applications. The metaphor of OS containers
being &#8220;shipping containers&#8221; has also been used widely&#8212;instead
of custom build and deployment scripts for each applications,
container (and VM) images provide standardized formats and
tool-chains.
<!--l. 145--><p class="indent" >  Another novel use-case of container images is to link container image
versions to the application software versions which they correspond to.
For example, Docker images can be automatically built whenever
changes to a source code repository are committed. This allows easier
and smoother continuous integration, since the changes in code base are
automatically reflected in the application images. An important
aspect of the distributed systems development process is updating
deployed services to reflect code changes, add features, etc. Rolling
updates of deployed containers is a feature which is exposed by
Kubernetes.
<a 
 id="x1-62r7"></a>
<!--l. 154--><p class="noindent" ><span 
class="ptmb8t-x-x-120">7.</span>  <span 
class="ptmb8t-x-x-120">MIXING CONTAINERS AND VMS</span><a 
 id="Q1-1-16"></a>
.
<!--l. 3--><p class="indent" >  Thus far we have seen that containers and VMs have performance and
manageability trade-offs. It is interesting to consider if it is possible to
combine the best characteristics of each platform into a single
architecture. We shall study the viability of two approaches which mix
OS-level and hardware virtualization: nested containers in VMs, and      lightweight VMs.
     <a 
 id="x1-63r1"></a>
     <!--l. 7--><p class="noindent" ><span 
class="ptmb8t-x-x-120">7.1</span>    <span 
class="ptmb8t-x-x-120">Containers inside VMs</span> <a 
 id="Q1-1-16"></a> 
.
     <!--l. 10--><p class="indent" >    In the nested container architecture, each container is encapsulated in
     a VM, and applications run inside the container. One or more
     containers may be encapsulated inside a VM. This nesting has several
     advantages. Applications can benefit from the security and performance
     isolation provided by the VM, and still take advantage of the
     provisioning and deployment aspects of containers. This approach
     is quite popular, and is used to run containers in public clouds
     where isolation and security are important concerns. Container
     services in public clouds, such as Amazon AWS Elastic Container
     Service&#x00A0;<span class="cite">[<span 
class="ptmb8t-x-x-90">?</span>]</span> and Google Cloud Platform&#8217;s Container Engine&#x00A0;<span class="cite">[<span 
class="ptmb8t-x-x-90">?</span>]</span>
     run user containers inside virtual machine instances which are
     re-purposed to also support container deployment stacks such as
     Docker. From a cloud and data center operator&#8217;s point of view, an
     advantage of this approach is that the existing hardware virtualization
     based architecture can be used for resource provisioning and
     management.
     <!--l. 12--><p class="indent" >    In addition to public cloud settings, nested containers in VMs are also
     useful on the desktop if the host operating system does not natively
     support containers, but can run virtual machines. Such a setup is the
     default way of running Docker on operating systems like Mac-OS
     (which cannot run Linux binaries, and uses Virtual-Box VMs to run
     Docker containers). This allows users and developers to run containers
     in their existing environment.
     <!--l. 14--><p class="indent" >    <hr class="figure"><div class="figure" 
>
                                                                                                
                                                                                                
                                                
                                                                                                
                                                                                                
<!--l. 16--><p class="noindent" ><img 
src="a1-sharma19x.png" alt="PIC" class="graphics"><!--tex4ht:graphics  
name="a1-sharma19x.png" src="../graphs/overcommit/15times.pdf"  
--> <a 
 id="x1-64r12"></a>
<span 
class="ptmb8t-x-x-90">Figure 12: Relative performance of VMs and nested containers</span>
<span 
class="ptmb8t-x-x-90">(LXCVM) for Kernel compile and YCSB workloads when both</span>
<span 
class="ptmb8t-x-x-90">CPU and memory are overcommitted by 1.5 times. Containers</span>
<span 
class="ptmb8t-x-x-90">inside VMs improve the running times of these workloads by</span>
<span 
class="ptmb8t-x-x-90">up to 5%.</span>
                                                
                                                                                                
                                                                                                
<!--l. 20--><p class="indent" >  </div><hr class="endfigure">
<!--l. 23--><p class="indent" >  Nested containers in VMs provide another, perhaps surprising
performance benefit. Since only containers from a single user may be
allowed to run in a VM, neighboring containers within a VM can now
be trusted. In addition to reducing the security risk, this also opens the
door for using <span 
class="ptmri8t-x-x-90">soft </span>resource limits. As noted in Section&#x00A0;<a 
href="#x1-46r1">5.1<!--tex4ht:ref: subsec:manage-alloc --></a>, soft
container resource limits allow containers to use under-utilized
resources, thereby increasing the application performance. In
Figure&#x00A0;<a 
href="#x1-64r12">12<!--tex4ht:ref: fig:lxcvm2 --></a>, we compare the performance of applications running on
LXC, VMs, and nested LXC containers inside a VM. In the case of
nested containers, the container resource limits (both CPU and memory)
are soft-limited, and we run multiple containers inside a VM. With VMs
and LXCs, we only run a single application in each container/VM. We
see that the running time of kernel-compile in nested containers
(LXCVM) is about 2% lower than compared to VMs, and the YCSB
read latency is lower by 5% compared to VMs. Thus, running larger
VMs and running soft-limited containers inside them <span 
class="ptmri8t-x-x-90">improves</span>
performance slightly when compared to running applications in separate
virtual machine silos.
<!--l. 25--><p class="noindent" ><span 
class="ptmb8t-x-x-90">Result: </span><span 
class="ptmri8t-x-x-90">Nesting containers in VMs can provide VM-like isolation for</span>
<span 
class="ptmri8t-x-x-90">applications, and enables the use of soft resource limits which provide</span>
<span 
class="ptmri8t-x-x-90">slightly improved performance compared to VMs.</span>
<a 
 id="x1-65r2"></a>
<!--l. 29--><p class="noindent" ><span 
class="ptmb8t-x-x-120">7.2</span>  <span 
class="ptmb8t-x-x-120">Lightweight VMs</span><a 
 id="Q1-1-17"></a>
.
<!--l. 31--><p class="indent" >  We have seen how nested containers combine VM-level isolation and
the deployment and provisioning aspects of containers. There is another
approach which attempts to provide similar functionality, but does not
rely on nesting. Instead, lightweight VM approaches like Clear
Linux&#x00A0;<span class="cite">[<span 
class="ptmb8t-x-x-90">?</span>]</span> and VMWare&#8217;s Project Bonneville&#x00A0;<span class="cite">[<span 
class="ptmb8t-x-x-90">?</span>]</span> seek to provide
extremely low overhead hardware virtualization. Lightweight VMs run
customized guest operating systems which are optimized for quick
boot-up, and are characterized by their low footprint and extensive
use of para-virtualized interfaces. Lightweight VMs therefore
provide reduced footprint and deployment times compared to
traditional hardware VMs. We will use the open source Clear
Linux system as a representative example of such a lightweight
VM.
<!--l. 34--><p class="indent" >  Lightweight VMs seek to address two of the major drawbacks of
traditional VMs: footprint, and host transparency. The VM footprint is
reduced by removing redundant functionality provided by the
hypervisor, such as bootloaders, emulation for legacy devices such as
floppy drives, etc. Along with optimizations for fast kernel boot, this
allows the VM to boot in under one second, compared to tens of seconds
required to boot traditional VMs. We measured the launch time of Clear
Linux Lightweight VMs to be under 0.8 seconds, compared to 0.3
seconds for the equivalent Docker container. While containers and
lightweight VMs can be started up faster than traditional VMs,
traditional VMs can also be quickly restored from existing snapshots
using lazy restore&#x00A0;<span class="cite">[<span 
class="ptmb8t-x-x-90">?</span>]</span>, or can be cloned from existing VMs&#x00A0;<span class="cite">[<span 
class="ptmb8t-x-x-90">?</span>]</span>.
Thus, instead of relying on a cold boot, fast restore and cloning
techniques can be applied to traditional VMs to achieve the same
effect.
<!--l. 37--><p class="indent" >  Containers can access files on the host file system directly, and
executables, libraries and data required for application operation does
not have to be first transferred to a virtual disk as is the case for
traditional VMs. As noted earlier, virtual disks decouple the host
and guest operation and pose an inconvenience. An important
feature of lightweight VMs like Clear Linux is that the VMs can
directly access host file system data. This eliminates the time
consuming step of creating bespoke virtual disk images for each
application. Instead, VMs can share and access files on the host file       system without going through the virtual disk abstraction. This is
     enabled by new technologies in Linux like Direct-Access (DAX),
     which allows true zero copy into the VM&#8217;s user address space and
     bypasses the page cache completely. In addition to removing the
     need for dedicated block-level virtual disks, zero-copy and page
     cache bypass reduces the memory footprint of these VMs by
     eliminating double caching&#x00A0;<span class="cite">[<span 
class="ptmb8t-x-x-90">?</span>]</span>. To translate guest file system
     operations for the host file system, Clear Linux uses the 9P file system
     interface&#x00A0;<span class="cite">[<span 
class="ptmb8t-x-x-90">?</span>]</span>.
     <!--l. 39--><p class="indent" >    This combination of lightweight hardware virtualization and direct
     host file system access enables new ways of combining hardware and
     operating system virtualization. For example, Clear Linux can run
     existing Docker containers and run them as lightweight VMs, thus
     providing security and performance isolation, and making VMs behave
     like containers as far as deployment goes.
     <!--l. 42--><p class="noindent" ><span 
class="ptmb8t-x-x-90">Result: </span><span 
class="ptmri8t-x-x-90">Lightweight VMs with host filesystem sharing reduces virtual</span>
     <span 
class="ptmri8t-x-x-90">machine overhead and footprint, providing container-like deployment</span>
     <span 
class="ptmri8t-x-x-90">with the isolation properties of VMs.</span>
     <a 
 id="x1-66r8"></a>
     <!--l. 157--><p class="noindent" ><span 
class="ptmb8t-x-x-120">8.</span>    <span 
class="ptmb8t-x-x-120">RELATED WORK</span> <a 
 id="Q1-1-17"></a> 
.
     <!--l. 3--><p class="indent" >    Both hardware and operating system virtualization have a long and
     storied history in computing. More recently, hypervisors such as
     Xen&#x00A0;<span class="cite">[<span 
class="ptmb8t-x-x-90">?</span>]</span>, VMware ESX&#x00A0;<span class="cite">[<span 
class="ptmb8t-x-x-90">?</span>]</span>, and KVM&#x00A0;<span class="cite">[<span 
class="ptmb8t-x-x-90">?</span>]</span> have been used to provide
     virtualized data centers. Operating system virtualization in UNIX has
     been originally implemented in FreeBSD using Jails&#x00A0;<span class="cite">[<span 
class="ptmb8t-x-x-90">?</span>]</span> and in Solaris
     using Zones&#x00A0;<span class="cite">[<span 
class="ptmb8t-x-x-90">?</span>]</span>. The recent surge of interest in containers has been
     partly because of the maturation of cgroups and namespaces in Linux,
     and partly because of Docker&#x00A0;<span class="cite">[<span 
class="ptmb8t-x-x-90">?</span>]</span>.
     <!--l. 6--><p class="indent" >    Both hardware and operating system virtualization technologies have
     been growing at a rapid pace, and research work evaluating the
     performance aspects of these platforms provides an empirical basis for
     comparing their performance. A performance study of container and
     virtual machine performance in Linux is performed in&#x00A0;<span class="cite">[<span 
class="ptmb8t-x-x-90">?</span>,&#x00A0;<span 
class="ptmb8t-x-x-90">?</span>]</span>, which
     evaluates LXC, Docker, and KVM across a wide spectrum of
     benchmarks. While conventional wisdom and folklore point to the
     lightweight nature of containers compared to VMs, recent work&#x00A0;<span class="cite">[<span 
class="ptmb8t-x-x-90">?</span>]</span> has
     investigated memory footprint of VMs with page-level memory
     deduplication&#x00A0;<span class="cite">[<span 
class="ptmb8t-x-x-90">?</span>,&#x00A0;<span 
class="ptmb8t-x-x-90">?</span>]</span> and shown that the effective memory footprint of
     VMs may not be as large as widely claimed. Containers have been
     pitched as a viable alternative to VMs in domains where strict isolation
     is not paramount such as high performance computing&#x00A0;<span class="cite">[<span 
class="ptmb8t-x-x-90">?</span>,&#x00A0;<span 
class="ptmb8t-x-x-90">?</span>]</span> and big
     data processing&#x00A0;<span class="cite">[<span 
class="ptmb8t-x-x-90">?</span>]</span>. Docker storage performance is investigated
     in&#x00A0;<span class="cite">[<span 
class="ptmb8t-x-x-90">?</span>]</span>.
     <!--l. 9--><p class="indent" >    The high performance overheads of hardware virtualization in certain
     situations have been well studied&#x00A0;<span class="cite">[<span 
class="ptmb8t-x-x-90">?</span>]</span>, and many approaches have been
     proposed to reduce the hypervisor overhead&#x00A0;<span class="cite">[<span 
class="ptmb8t-x-x-90">?</span>]</span>. Operating system
     virtualization has always promised to deliver low overhead isolation,
     and comparing the two approaches to virtualization is covered in
     &#x00A0;<span class="cite">[<span 
class="ptmb8t-x-x-90">?</span>,&#x00A0;<span 
class="ptmb8t-x-x-90">?</span>]</span>. While most prior work on comparing performance of
     containers vs. VMs has focused on a straight shoot-out between
     the two, our work also considers performance interference&#x00A0;<span class="cite">[<span 
class="ptmb8t-x-x-90">?</span>]</span>,
     overcommitment, and different types of resource limits. Isolation in
     multi-tenants environments is crucial, especially in public clouds which
     have begun offering container services in addition to conventional VM
     based servers&#x00A0;<span class="cite">[<span 
class="ptmb8t-x-x-90">?</span>,&#x00A0;<span 
class="ptmb8t-x-x-90">?</span>,&#x00A0;<span 
class="ptmb8t-x-x-90">?</span>]</span>. In addition to evaluating the performance of
     the two virtualization technologies, we also show the performance
     of containers inside VMs and lightweight VMs such as Clear
     Linux&#x00A0;<span class="cite">[<span 
class="ptmb8t-x-x-90">?</span>]</span>.
     <!--l. 11--><p class="indent" >    Unikernels&#x00A0;<span class="cite">[<span 
class="ptmb8t-x-x-90">?</span>]</span> have been proposed as another virtualization platform
     that run applications directly on hardware virtual machines without a
     guest OS. Their value in production environments is still being
                                                                                                
                                                                                                
determined&#x00A0;<span class="cite">[<span 
class="ptmb8t-x-x-90">?</span>]</span>.
<!--l. 13--><p class="indent" >  Both VMs and containers need to be deployed across data
centers at large scale, and we have also compared the capabilities of
their respective management frameworks. Frameworks such as
OpenStack&#x00A0;<span class="cite">[<span 
class="ptmb8t-x-x-90">?</span>]</span>, vCenter&#x00A0;<span class="cite">[<span 
class="ptmb8t-x-x-90">?</span>]</span> provide the management functionality for
VMs. Kubernetes&#x00A0;<span class="cite">[<span 
class="ptmb8t-x-x-90">?</span>]</span>, Docker Swarm&#x00A0;<span class="cite">[<span 
class="ptmb8t-x-x-90">?</span>]</span>, Mesos&#x00A0;<span class="cite">[<span 
class="ptmb8t-x-x-90">?</span>]</span>, Marathon&#x00A0;<span class="cite">[<span 
class="ptmb8t-x-x-90">?</span>]</span>,
and many other burgeoning frameworks provide equivalent functionality
for containers.
<a 
 id="x1-67r9"></a>
<!--l. 160--><p class="noindent" ><span 
class="ptmb8t-x-x-120">9.</span>  <span 
class="ptmb8t-x-x-120">CONCLUSIONS</span><a 
 id="Q1-1-17"></a>
.
<!--l. 3--><p class="indent" >  Containers and VMs differ in the virtualization technology they use,
and this difference manifests itself in their performance, manageability,
and use-cases. Containers promise bare metal performance, but as we
have shown, they may suffer from performance interference in
multi-tenant scenarios. Containers share the underlying OS kernel, and
this contributes to the lack of isolation. Unlike VMs, which have strict
resource limits, the containers also allow soft limits, which are helpful
in overcommitment scenarios, since they may use underutilized
resources allocated to other containers. The lack of isolation and more
efficient resource sharing due to soft-limits makes running containers
inside VMs a viable architecture.
<!--l. 6--><p class="indent" >  While containers may offer near bare metal performance and
a low footprint, their popularity has also been fueled by their
integration into the software development process. In particular,
Docker&#8217;s use of copy-on-write layered file systems and version
control has enabled easier continuous delivery and integration
and a more agile software development process. Lightweight
VMs such as Clear Linux aim to provide the popular container
features, but with the isolation properties of VMs, and hybrid
virtualization approaches seems to be a promising avenue for
research.
<!--l. 9--><p class="noindent" ><span 
class="ptmb8t-x-x-90">Acknowledgements. </span>We thank all the reviewers for their insightful
comments, which improved the quality of this paper. This work is
supported in part by NSF grants #1422245 and #1229059 and
a gift from Cisco. Any opinions, findings, and conclusions or
recommendations expressed in this paper are those of the authors and do
not necessarily reflect the views of the funding agencies.


                                                
                                                                                                


