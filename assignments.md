---
layout: page
title: Assignments
permalink: /assignments/
---
This course consists of four lab assignments, three reading assignments, and one final project. All the assignments/project should be done in groups of two and have to be submitted before their deadlines. The points are distributed accordingly:
<ul>
<li>Four lab assignments (30 points)</li>
<li>Three reading assignments (15 points)</li>
<li>The final project (15 points)</li>
<li>The final exam (40 points)</li>
</ul>

<br>
<hr>
<br>
# **Lab Assignments**
The lab assignments span over different topics of the course. For each lab, a zip file is given that includes instructions for doing the assignment.

* **Lab 1:** in the first lab assignment you will practice the basics of the data intensive programming by setting up HDFS and Hadoop MapReduce and implementing a simple application on them (7 points)
  - Download [[lab1](https://www.dropbox.com/s/whu2b7s5dhp3skw/lab1.zip?dl=0)]
  - Deadline 2018/09/23
* **Lab 2:** the second lab assignment is based of the Spark and Spark SQL, and you will learn how to use them to process and analyze data (8 points).
  - Download [lab2]
  - Deadline 2018/09/30
* **Lab 3:** the concentration of this lab assignment is to process streaming data. You will work with Spark Streaming in this lab to process data on-line as it flows (8 points).
  - Download [lab3]
  - Deadline 2018/10/07
* **Lab 4:** in the last assignment you will work with GraphX to process graph-based data (7 points).
  - Download [lab4]
  - Deadline 2018/10/14

<br>
<hr>
<br>
# **Reading Assignments**
For each reading assignment, you are asked to read two papers and write a short review (max. three pages). Each reading assignment has five points. The reviews should have the following sections:
* **Motivation:** describe the motivation of the paper, and explain WHY the addressed problem is interesting and important to be solved.
* **Contributions:** explain the main contributions of the paper, and present WHAT the solved problems are.
* **Solution:** very briefly explain HOW the authors solve the mentioned problems.
* **You opinion:** compare the two papers and explain how they are related to/different from each other.

### General reading instruction
* How to read a paper [[pdf]](/papers/paper-reading.pdf)
* How to write a review [[pdf]](/papers/review-writing.pdf)
* It is strongly recommended to use the given templates [[latex]](/papers/latex_template.tex) [[word]](/papers/word_template.doc)

### Papers
* **Reading assignmet 1:** GFS and MapReduce
  - The Google File System [[pdf]](/papers/2003 - The Google File System (SOSP).pdf)
  - MapReduce Simplifed Data Processing on Large Clusters [[pdf]](/papers/2004 - MapReduce  Simplifed Data Processing on Large Clusters (OSDI).pdf)
  - Deadline 2018/09/24
* **Reading assignmet 2:** Spark and Spark SQL
  - Resilient Distributed Datasets: A Fault-Tolerant Abstraction for In-Memory Cluster Computing [[pdf]](/papers/2012 - Resilient Distributed Datasets: A Fault-Tolerant Abstraction for In-Memory Cluster Computing (NSDI).pdf)
  - Spark SQL: Relational Data Processing in Spark [[pdf]](/papers/2015 - Spark SQL - Relational Data Processing in Spark (SIGMOD).pdf)
  - Deadline 2018/10/05
* **Reading assignmet 3:** Spark Streaming and Flink
  - Discretized Streams: Fault-Tolerant Streaming Computation at Scale [[pdf]](/papers/2013 - Discretized Streams: Fault-Tolerant Streaming Computation at Scale (SOSP).pdf)
  - Apache Flink: Stream and Batch Processing in a Single Engine [[pdf]](/papers/2015 - Apache Flink: Stream and Batch Processing in a Single Engine.pdf)
  - Deadline 2018/10/14

<br>
<hr>
<br>
# **Project**
You should define your own project by writing at most one page description of the project, and getting your project proposal approved by the examiner. The project proposal should cover the following headings:
* **Problem description:** what is the problem that you will be investigating?
* **Tools:** what tools you are going to use? In the course we mainly used Spark, but you are free to explore new tools and technologies.
* **Data:** what data will you use and how are you going to collect it? 
* **Methodology and algorithm:** what method(s) or algorithm(s) are you proposing? 

### What to deliver
You can implement your code using Jupyter Notebook or as a stand alone application. You should submit a zip file containing your code and a short report (two to three pages) about what you have done, the dataset, your method, your results, and how to run the code.
